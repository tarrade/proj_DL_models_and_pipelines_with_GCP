{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIXED: Issue 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue https://github.com/tensorflow/tensorflow/issues/27696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0516 17:03:53.563227 4582979008 cross_device_ops.py:1182] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\n",
      "I0516 17:03:53.564509 4582979008 run_config.py:558] Initializing RunConfig with distribution strategies.\n",
      "I0516 17:03:53.565056 4582979008 estimator_training.py:167] Not using Distribute Coordinator.\n",
      "W0516 17:03:53.567290 4582979008 estimator.py:1811] Using temporary folder as model directory: /var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmpct2ql_wg\n",
      "I0516 17:03:53.568195 4582979008 keras.py:424] Using the Keras model provided.\n",
      "I0516 17:03:54.406841 4582979008 estimator.py:209] Using config: {'_model_dir': '/var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmpct2ql_wg', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0xb28d7a3c8>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb28d7a668>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n",
      "I0516 17:03:54.408506 4582979008 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I0516 17:03:54.409472 4582979008 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I0516 17:03:54.410520 4582979008 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "I0516 17:03:54.413426 4582979008 dataset_builder.py:174] Overwrite dataset info from restored data version.\n",
      "I0516 17:03:54.419337 4582979008 dataset_info.py:370] Field info.location from disk and from code do not match. Keeping the one from code.\n",
      "I0516 17:03:54.420114 4582979008 dataset_builder.py:216] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n",
      "I0516 17:03:55.062092 123145566818304 estimator.py:1145] Calling model_fn.\n",
      "I0516 17:03:55.368992 4582979008 cross_device_ops.py:393] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.374510 4582979008 cross_device_ops.py:393] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.379673 4582979008 cross_device_ops.py:393] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.384248 4582979008 cross_device_ops.py:393] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.745679 4582979008 cross_device_ops.py:393] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.747570 4582979008 cross_device_ops.py:393] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.749017 4582979008 cross_device_ops.py:393] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.750385 4582979008 cross_device_ops.py:393] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.752074 4582979008 cross_device_ops.py:393] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.753700 4582979008 cross_device_ops.py:393] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.755379 4582979008 cross_device_ops.py:393] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.757021 4582979008 cross_device_ops.py:393] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.837267 4582979008 cross_device_ops.py:393] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.842932 4582979008 cross_device_ops.py:393] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.851694 4582979008 cross_device_ops.py:393] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.861177 4582979008 cross_device_ops.py:393] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.866070 4582979008 cross_device_ops.py:393] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.869249 4582979008 cross_device_ops.py:393] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "I0516 17:03:55.883879 123145566818304 estimator.py:1147] Done calling model_fn.\n",
      "I0516 17:03:55.979688 4582979008 estimator.py:1364] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmpct2ql_wg/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "I0516 17:03:55.980520 4582979008 warm_starting_util.py:419] Warm-starting from: ('/var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmpct2ql_wg/keras/keras_model.ckpt',)\n",
      "I0516 17:03:55.981160 4582979008 warm_starting_util.py:333] Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "I0516 17:03:56.040284 4582979008 warm_starting_util.py:481] Warm-started 8 variables.\n",
      "I0516 17:03:56.042701 4582979008 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0516 17:03:56.654402 4582979008 monitored_session.py:240] Graph was finalized.\n",
      "I0516 17:03:56.837654 4582979008 session_manager.py:500] Running local_init_op.\n",
      "I0516 17:03:56.870195 4582979008 session_manager.py:502] Done running local_init_op.\n",
      "I0516 17:03:57.601773 4582979008 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmpct2ql_wg/model.ckpt.\n",
      "I0516 17:03:59.517050 4582979008 basic_session_run_hooks.py:262] loss = 2.8022377, step = 0\n",
      "I0516 17:04:00.343311 4582979008 basic_session_run_hooks.py:606] Saving checkpoints for 25 into /var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmpct2ql_wg/model.ckpt.\n",
      "I0516 17:04:00.596259 4582979008 dataset_builder.py:174] Overwrite dataset info from restored data version.\n",
      "I0516 17:04:00.602108 4582979008 dataset_info.py:370] Field info.location from disk and from code do not match. Keeping the one from code.\n",
      "I0516 17:04:00.606704 4582979008 dataset_builder.py:216] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n",
      "I0516 17:04:00.872245 4582979008 estimator.py:1145] Calling model_fn.\n",
      "I0516 17:04:01.115833 4582979008 estimator.py:1147] Done calling model_fn.\n",
      "I0516 17:04:01.171324 4582979008 evaluation.py:255] Starting evaluation at 2019-05-16T17:04:01Z\n",
      "I0516 17:04:01.502079 4582979008 monitored_session.py:240] Graph was finalized.\n",
      "W0516 17:04:01.503654 4582979008 deprecation.py:323] From /Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "I0516 17:04:01.507088 4582979008 saver.py:1280] Restoring parameters from /var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmpct2ql_wg/model.ckpt-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0516 17:04:01.634721 4582979008 session_manager.py:500] Running local_init_op.\n",
      "I0516 17:04:01.698223 4582979008 session_manager.py:502] Done running local_init_op.\n",
      "I0516 17:04:03.411951 4582979008 evaluation.py:167] Evaluation [1/5]\n",
      "I0516 17:04:03.420993 4582979008 evaluation.py:167] Evaluation [2/5]\n",
      "I0516 17:04:03.429873 4582979008 evaluation.py:167] Evaluation [3/5]\n",
      "I0516 17:04:03.439300 4582979008 evaluation.py:167] Evaluation [4/5]\n",
      "I0516 17:04:03.448176 4582979008 evaluation.py:167] Evaluation [5/5]\n",
      "I0516 17:04:03.507338 4582979008 evaluation.py:275] Finished evaluation at 2019-05-16-17:04:03\n",
      "I0516 17:04:03.508151 4582979008 estimator.py:2039] Saving dict for global step 25: accuracy = 0.63125, global_step = 25, loss = 1.6425345\n",
      "I0516 17:04:03.598064 4582979008 estimator.py:2099] Saving 'checkpoint_path' summary for global step 25: /var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmpct2ql_wg/model.ckpt-25\n",
      "I0516 17:04:03.687540 4582979008 estimator.py:368] Loss for final step: 0.67927635.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.63125, 'loss': 1.6425345, 'global_step': 25}, [])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "# Define the estimator's input_fn\n",
    "STEPS_PER_EPOCH = 5\n",
    "#BUFFER_SIZE = 10 # Use a much larger value for real code. \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "    mnist_train, mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "    \n",
    "        return image, label[..., tf.newaxis]\n",
    "\n",
    "    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    return train_data.repeat()\n",
    "\n",
    "# Define train & eval specs\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=input_fn,\n",
    "                                    max_steps=STEPS_PER_EPOCH * NUM_EPOCHS)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=input_fn,\n",
    "                                  steps=STEPS_PER_EPOCH)\n",
    "\n",
    "def make_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#####\n",
    "## works\n",
    "#strategy=None \n",
    "## crash\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# config tf.estimator to use a give strategy\n",
    "training_config = tf.estimator.RunConfig(train_distribute=strategy)\n",
    "#####\n",
    "\n",
    "estimator = tf.keras.estimator.model_to_estimator(\n",
    "    keras_model = model,\n",
    "    config=training_config\n",
    ")\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIXED Issue 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue https://github.com/tensorflow/tensorflow/issues/27581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0516 15:55:46.509448 4631774656 dataset_builder.py:174] Overwrite dataset info from restored data version.\n",
      "I0516 15:55:46.511722 4631774656 dataset_info.py:370] Field info.location from disk and from code do not match. Keeping the one from code.\n",
      "I0516 15:55:46.512466 4631774656 dataset_builder.py:216] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                346176    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 347,402\n",
      "Trainable params: 347,274\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n",
      "train\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 453ms/step - loss: 1.4497 - accuracy: 0.5719\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.9702 - accuracy: 0.6750\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.7482 - accuracy: 0.7719\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.5709 - accuracy: 0.8562\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.5389 - accuracy: 0.8562\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.5731 - accuracy: 0.8344\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4622 - accuracy: 0.8969\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3518 - accuracy: 0.9219\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.2871 - accuracy: 0.9563\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.3665 - accuracy: 0.9187\n",
      "evaluate\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4394 - accuracy: 0.7812\n",
      "predict on batch\n",
      "[[0.09913913 0.03892732 0.33971262 0.07623772 0.05127708 0.04202563\n",
      "  0.13012382 0.04549295 0.11951347 0.05755023]\n",
      " [0.58329886 0.01284418 0.06186528 0.05517435 0.01576896 0.05449204\n",
      "  0.03106456 0.04741744 0.07669283 0.06138153]\n",
      " [0.15171182 0.02285899 0.07202249 0.10229845 0.04133021 0.18743268\n",
      "  0.0727735  0.05858784 0.18861885 0.10236517]\n",
      " [0.11634745 0.03468547 0.05774639 0.0919371  0.04291645 0.06664032\n",
      "  0.06748458 0.34197158 0.09626064 0.0840101 ]\n",
      " [0.13743342 0.0224991  0.05700003 0.04901872 0.06409317 0.0487637\n",
      "  0.07909202 0.10329906 0.16135474 0.2774461 ]\n",
      " [0.09268443 0.1423252  0.1151204  0.11038071 0.06009423 0.07486234\n",
      "  0.1021251  0.08194935 0.13533147 0.08512679]\n",
      " [0.13244726 0.02318298 0.08620345 0.3110404  0.04082269 0.09651461\n",
      "  0.04879926 0.05641791 0.13892162 0.06564982]\n",
      " [0.13234231 0.02387219 0.04613722 0.05942336 0.11340055 0.08992005\n",
      "  0.08203672 0.09640002 0.11044028 0.24602737]\n",
      " [0.17226522 0.022041   0.05059194 0.04306937 0.05972058 0.1864066\n",
      "  0.08463561 0.09864194 0.19014029 0.09248743]\n",
      " [0.10040943 0.05325584 0.34761146 0.08984379 0.06313676 0.04765578\n",
      "  0.11804664 0.05452728 0.07235096 0.05316209]\n",
      " [0.13761497 0.02924865 0.05980809 0.05121346 0.04269152 0.04688377\n",
      "  0.05788566 0.3205257  0.11251157 0.14161666]\n",
      " [0.13472179 0.01584179 0.05378629 0.04555129 0.25504103 0.06201949\n",
      "  0.09041668 0.0624736  0.13206822 0.14807986]\n",
      " [0.13668281 0.02653342 0.08573093 0.14280792 0.07744472 0.1397601\n",
      "  0.10374643 0.0801632  0.14432676 0.06280369]\n",
      " [0.19374065 0.01601888 0.11640032 0.11737335 0.04401091 0.04858353\n",
      "  0.04668649 0.02931854 0.31593224 0.07193501]\n",
      " [0.15474735 0.02262986 0.40508595 0.12513302 0.03428236 0.03576357\n",
      "  0.04774231 0.03957842 0.08187649 0.0531607 ]\n",
      " [0.11752994 0.04487301 0.15968578 0.12235824 0.05644162 0.07837907\n",
      "  0.12909588 0.12258625 0.09165043 0.07739974]\n",
      " [0.11718724 0.03692823 0.06987412 0.07186621 0.10855143 0.05561427\n",
      "  0.06660184 0.09794419 0.15694782 0.21848474]\n",
      " [0.12447672 0.03618249 0.09695509 0.05388304 0.04182955 0.04475341\n",
      "  0.07380544 0.24228692 0.10459602 0.18123135]\n",
      " [0.1306599  0.02317812 0.07352921 0.08270852 0.03547468 0.03984092\n",
      "  0.0616413  0.35610294 0.10716859 0.08969578]\n",
      " [0.1634308  0.01964851 0.08115257 0.06360025 0.04180905 0.0605672\n",
      "  0.3300886  0.03615463 0.12166037 0.08188808]\n",
      " [0.3850058  0.01768001 0.10187371 0.07399346 0.04028275 0.04266949\n",
      "  0.12702166 0.05172763 0.06779785 0.0919476 ]\n",
      " [0.11661554 0.04416868 0.18427482 0.08547236 0.06779249 0.07430914\n",
      "  0.09756541 0.07459448 0.13475989 0.12044718]\n",
      " [0.09611341 0.06687735 0.12074985 0.17334472 0.06350331 0.07569929\n",
      "  0.08795872 0.06018136 0.18599834 0.06957369]\n",
      " [0.14210033 0.02383578 0.06352208 0.05982513 0.13097173 0.04129719\n",
      "  0.06696    0.06094159 0.12867998 0.28186616]\n",
      " [0.11192048 0.04041713 0.4128687  0.11177073 0.03185366 0.03860202\n",
      "  0.05477418 0.03193495 0.11969234 0.04616578]\n",
      " [0.12904467 0.02495018 0.03918919 0.06130943 0.11257367 0.0548614\n",
      "  0.07666249 0.07253725 0.12003136 0.30884036]\n",
      " [0.2191596  0.01730795 0.07097674 0.08526107 0.06834481 0.07783454\n",
      "  0.12973535 0.05665911 0.1814581  0.09326271]\n",
      " [0.08545713 0.11649848 0.11044416 0.09358463 0.07421916 0.10099126\n",
      "  0.10406812 0.07018332 0.15943877 0.08511508]\n",
      " [0.11336641 0.04581955 0.0664361  0.0953618  0.16475618 0.09303871\n",
      "  0.08165311 0.06109592 0.11341832 0.16505395]\n",
      " [0.12312837 0.03443385 0.09638535 0.35650808 0.0377687  0.08641212\n",
      "  0.06455424 0.03197678 0.11855678 0.05027569]\n",
      " [0.12087133 0.02564524 0.08347051 0.06427943 0.17627372 0.08689313\n",
      "  0.10257354 0.06748109 0.1257564  0.14675562]\n",
      " [0.20508976 0.02158964 0.07918231 0.08251404 0.1962162  0.05171524\n",
      "  0.07111966 0.04231759 0.08257955 0.167676  ]\n",
      " [0.17092837 0.01959864 0.09257845 0.18961054 0.04984701 0.11616271\n",
      "  0.04388412 0.04827255 0.20876476 0.06035288]\n",
      " [0.0847585  0.07363779 0.08914897 0.13839073 0.0794895  0.08226694\n",
      "  0.09975933 0.08060239 0.17312323 0.09882262]\n",
      " [0.09923198 0.04180484 0.11779869 0.08103705 0.04860699 0.05320135\n",
      "  0.34469655 0.04126028 0.0802509  0.09211145]\n",
      " [0.1240226  0.02308239 0.05257693 0.06455989 0.16133408 0.0572712\n",
      "  0.06996302 0.0761079  0.12745884 0.24362311]\n",
      " [0.35746932 0.01717643 0.06154613 0.09888743 0.04870155 0.06533317\n",
      "  0.08137549 0.04430734 0.12188488 0.10331833]\n",
      " [0.13528694 0.02867179 0.12650931 0.29781318 0.04341235 0.07395249\n",
      "  0.05666277 0.03519236 0.16120303 0.04129577]\n",
      " [0.12745537 0.02493614 0.08822    0.04447456 0.04457726 0.03291491\n",
      "  0.4366347  0.04026937 0.05186949 0.10864823]\n",
      " [0.525157   0.00829212 0.04886051 0.05419271 0.02398637 0.05566241\n",
      "  0.0718446  0.0393423  0.10794821 0.06471372]\n",
      " [0.3724792  0.01060275 0.09914038 0.05187941 0.06553987 0.04061538\n",
      "  0.09887192 0.05471798 0.11485583 0.09129725]\n",
      " [0.06967206 0.13763522 0.15551077 0.10959049 0.07199797 0.06714526\n",
      "  0.08678103 0.09002958 0.12781595 0.08382162]\n",
      " [0.11750882 0.02918438 0.06332288 0.07291402 0.08952954 0.05985883\n",
      "  0.08383256 0.09579623 0.11236965 0.27568308]\n",
      " [0.19785939 0.02692463 0.17968905 0.11808892 0.07402223 0.04606028\n",
      "  0.1360391  0.03167543 0.09846207 0.09117886]\n",
      " [0.0997229  0.02748105 0.12665549 0.3329023  0.03988421 0.06882936\n",
      "  0.04390426 0.03735511 0.14862251 0.07464288]\n",
      " [0.14534102 0.01477509 0.07469919 0.03557857 0.03311507 0.04103833\n",
      "  0.53075194 0.02523319 0.03257748 0.06689014]\n",
      " [0.11570077 0.02788336 0.07648364 0.08454575 0.09661377 0.04792352\n",
      "  0.06712962 0.08315068 0.15158592 0.24898298]\n",
      " [0.08726481 0.15903734 0.13142595 0.09502514 0.0538109  0.077952\n",
      "  0.09641374 0.07790105 0.12041079 0.10075831]\n",
      " [0.13697797 0.02281318 0.09302451 0.20596498 0.03688404 0.07091213\n",
      "  0.06513786 0.22559284 0.08532882 0.05736363]\n",
      " [0.15870778 0.01923451 0.07138527 0.04933402 0.0617793  0.03869785\n",
      "  0.35829702 0.038596   0.1067804  0.09718783]\n",
      " [0.5219413  0.0089669  0.05236077 0.10070535 0.0192462  0.05699945\n",
      "  0.08523075 0.03064634 0.07068371 0.05321926]\n",
      " [0.13907723 0.02280504 0.0518055  0.04819383 0.11952882 0.0362006\n",
      "  0.06581478 0.07168438 0.12378077 0.32110903]\n",
      " [0.0882413  0.16705579 0.10992138 0.10106152 0.05689785 0.05869076\n",
      "  0.09569727 0.08472533 0.13101077 0.10669801]\n",
      " [0.12935771 0.03789555 0.31482235 0.08662822 0.06052691 0.04095718\n",
      "  0.11969773 0.04857826 0.09931877 0.06221722]\n",
      " [0.45145792 0.01169267 0.0882455  0.07559567 0.04871088 0.04571356\n",
      "  0.07277952 0.04428277 0.08264211 0.07887945]\n",
      " [0.16712369 0.03273552 0.16705738 0.08538309 0.06657412 0.03445571\n",
      "  0.09858491 0.12019755 0.10808545 0.11980257]\n",
      " [0.14105308 0.02948072 0.10013724 0.08590814 0.07214404 0.11592497\n",
      "  0.27770096 0.03294018 0.083197   0.06151368]\n",
      " [0.14804062 0.02038518 0.07542225 0.0525352  0.20470825 0.05935717\n",
      "  0.09586315 0.05385927 0.16216297 0.12766603]\n",
      " [0.11181132 0.01686223 0.0743975  0.05205848 0.07527505 0.06037542\n",
      "  0.07082645 0.06318101 0.37622562 0.0989869 ]\n",
      " [0.15142141 0.02397321 0.08335695 0.07186858 0.05421619 0.0679974\n",
      "  0.3684556  0.03638791 0.05637726 0.08594549]\n",
      " [0.56060946 0.01360602 0.05760467 0.0345814  0.02102126 0.04854275\n",
      "  0.07497465 0.04860799 0.04880054 0.09165122]\n",
      " [0.0848608  0.13927601 0.13550386 0.092258   0.05485879 0.06794675\n",
      "  0.09024834 0.07790786 0.1596431  0.09749649]\n",
      " [0.0814392  0.13150099 0.134289   0.09924927 0.06900293 0.04993877\n",
      "  0.07977611 0.09516457 0.15884285 0.10079636]\n",
      " [0.11280397 0.02762683 0.10719502 0.11151679 0.06408828 0.06894302\n",
      "  0.09026606 0.04848941 0.2766189  0.09245177]]\n",
      "test on batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4605696, 0.765625]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "# Define the estimator's input_fn\n",
    "STEPS_PER_EPOCH = 5\n",
    "#BUFFER_SIZE = 10 # Use a much larger value for real code. \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "    mnist_train, mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "    \n",
    "        return image, label[..., tf.newaxis]\n",
    "\n",
    "    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    return train_data.repeat()\n",
    "\n",
    "\n",
    "def make_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "training_dataset=input_fn()\n",
    "\n",
    "print(\"train\")\n",
    "model.fit(training_dataset,\n",
    "          steps_per_epoch=5,\n",
    "          epochs=10,\n",
    "          verbose = 1)\n",
    "\n",
    "print(\"evaluate\")\n",
    "model.evaluate(training_dataset,\n",
    "              steps=1)\n",
    "\n",
    "print(\"predict on batch\")\n",
    "print(model.predict_on_batch(training_dataset))\n",
    "\n",
    "print(\"test on batch\")\n",
    "print(model.test_on_batch(training_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0516 16:14:02.396586 4631774656 keras.py:424] Using the Keras model provided.\n",
      "I0516 16:14:02.399237 4631774656 estimator.py:209] Using config: {'_model_dir': '/tmp/test2', '_tf_random_seed': None, '_save_summary_steps': 10, '_save_checkpoints_steps': 10, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb35e58080>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I0516 16:14:02.400191 4631774656 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I0516 16:14:02.400945 4631774656 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I0516 16:14:02.402135 4631774656 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 10 or save_checkpoints_secs None.\n",
      "I0516 16:14:02.407902 4631774656 estimator.py:360] Skipping training since max_steps has already saved.\n",
      "W0516 16:14:02.419650 4631774656 event_accumulator.py:344] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0516 16:14:02.421156 4631774656 event_accumulator.py:352] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "I0516 16:14:02.431520 4631774656 directory_watcher.py:118] No path found after /tmp/test2/events.out.tfevents.1558015778.Fabien-Tarrades-MacBook-Pro.local\n",
      "I0516 16:14:02.442379 4631774656 directory_watcher.py:118] No path found after /tmp/test2/eval/events.out.tfevents.1558015783.Fabien-Tarrades-MacBook-Pro.local\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train the model to estimator\n",
      "let inspect events.out.tfevents.* files\n",
      " \n",
      "training:\n",
      "dict_keys(['global_step/sec', 'loss_1'])\n",
      "(array([  1,  11,  21,  31,  41,  51,  61,  71,  81,  91, 101, 111, 121,\n",
      "       131, 141, 151, 161, 171, 181, 191, 201, 211, 221, 231, 241, 251,\n",
      "       261, 271, 281, 291, 301, 311, 321, 331, 341, 351, 361, 371, 381,\n",
      "       391, 401, 411, 421, 431, 441, 451, 461, 471, 481, 491, 501, 511,\n",
      "       521, 531, 541, 551, 561, 571, 581, 591, 601, 611, 621, 631, 641,\n",
      "       651, 661, 671, 681, 691, 701, 711, 721, 731, 741, 751, 761, 771,\n",
      "       781, 791, 801, 811, 821, 831, 841, 851, 861, 871, 881, 891, 901,\n",
      "       911, 921, 931, 941, 951, 961, 971, 981, 991]), array([2.7309413 , 0.68864745, 0.41577101, 0.38069659, 0.59798241,\n",
      "       0.28614211, 0.35943288, 0.22999889, 0.35692346, 0.2526049 ,\n",
      "       0.26848871, 0.11759079, 0.22881377, 0.22207737, 0.15091828,\n",
      "       0.17887621, 0.14436665, 0.11224867, 0.15940423, 0.17908514,\n",
      "       0.14958209, 0.13704482, 0.08541849, 0.245132  , 0.12384655,\n",
      "       0.16037495, 0.18672626, 0.12825429, 0.05761952, 0.05794587,\n",
      "       0.16425045, 0.22477531, 0.14909473, 0.18388332, 0.13857523,\n",
      "       0.08696433, 0.09388493, 0.06511007, 0.11632793, 0.1939145 ,\n",
      "       0.06919286, 0.13504821, 0.07562436, 0.0981548 , 0.09330794,\n",
      "       0.10033708, 0.15867695, 0.11395258, 0.1585142 , 0.10341614,\n",
      "       0.17447014, 0.14233407, 0.12466922, 0.07529552, 0.05918124,\n",
      "       0.12637994, 0.09865499, 0.03608365, 0.06622627, 0.20090328,\n",
      "       0.14791992, 0.17367935, 0.08469177, 0.09582721, 0.06227016,\n",
      "       0.05570736, 0.08851279, 0.1077071 , 0.10871401, 0.11764355,\n",
      "       0.10519619, 0.0436363 , 0.08139855, 0.08504344, 0.09256243,\n",
      "       0.13013689, 0.07181678, 0.1013051 , 0.16493595, 0.21693762,\n",
      "       0.09166117, 0.10654388, 0.12675862, 0.18374588, 0.08521944,\n",
      "       0.0581877 , 0.12632066, 0.06614725, 0.06810905, 0.05423842,\n",
      "       0.11764562, 0.0303425 , 0.05472765, 0.06969263, 0.02919581,\n",
      "       0.079943  , 0.09643405, 0.24732649, 0.02426332, 0.07674209]))\n",
      " \n",
      "evaluation:\n",
      "dict_keys(['accuracy', 'loss'])\n",
      "[1.85098433 0.17288671]\n",
      "[0.515625 0.96875 ]\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import logging\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import numpy as np\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "# Define the estimator's input_fn\n",
    "STEPS_PER_EPOCH = 5\n",
    "#BUFFER_SIZE = 10 # Use a much larger value for real code. \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "    mnist_train, mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "    \n",
    "        return image, label[..., tf.newaxis]\n",
    "\n",
    "    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    return train_data.repeat()\n",
    "\n",
    "def make_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# config tf.estimator to use a give strategy\n",
    "training_config = tf.estimator.RunConfig(model_dir='/tmp/test2',\n",
    "                                         save_summary_steps=10,  # save summary every n steps\n",
    "                                         save_checkpoints_steps=10,  # save model every iteration (needed for eval)\n",
    "                                         # save_checkpoints_secs=10,\n",
    "                                         keep_checkpoint_max=3,  # keep last n models\n",
    "                                         log_step_count_steps=50)\n",
    "\n",
    "# Define train & eval specs\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=input_fn,\n",
    "                                    max_steps=1000)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=input_fn,\n",
    "                                  steps=1)\n",
    "\n",
    "estimator = tf.keras.estimator.model_to_estimator(\n",
    "    keras_model = model,\n",
    "    config=training_config\n",
    ")\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "print('train the model to estimator')\n",
    "#estimator.train(input_fn=input_fn,\n",
    "#               steps=1000)\n",
    "\n",
    "def load_data_tensorboard(path):\n",
    "    event_acc = event_accumulator.EventAccumulator(path)\n",
    "    event_acc.Reload()\n",
    "    data = {}\n",
    "\n",
    "    for tag in sorted(event_acc.Tags()[\"scalars\"]):\n",
    "        x, y = [], []\n",
    "        for scalar_event in event_acc.Scalars(tag):\n",
    "            x.append(scalar_event.step)\n",
    "            y.append(scalar_event.value)\n",
    "        data[tag] = (np.asarray(x), np.asarray(y))\n",
    "    return data\n",
    "\n",
    "print('let inspect events.out.tfevents.* files')\n",
    "print(' ')\n",
    "print('training:')\n",
    "history_train=load_data_tensorboard('/tmp/test2/')\n",
    "print(history_train.keys())\n",
    "print(history_train['loss_1'])\n",
    "print(' ')\n",
    "print('evaluation:')\n",
    "history_eval=load_data_tensorboard('/tmp/test2/eval')\n",
    "print(history_eval.keys())\n",
    "print(history_eval['loss'][1])\n",
    "print(history_eval['accuracy'][1])\n",
    "print()\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28576\r\n",
      "drwxr-xr-x  16 tarrade  wheel      512 May 16 16:10 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxrwxrwt  14 root     wheel      448 May 16 16:09 \u001b[30m\u001b[42m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  wheel      177 May 16 16:10 checkpoint\r\n",
      "drwxr-xr-x   3 tarrade  wheel       96 May 16 16:09 \u001b[34meval\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  wheel   797458 May 16 16:10 events.out.tfevents.1558015778.Fabien-Tarrades-MacBook-Pro.local\r\n",
      "-rw-r--r--   1 tarrade  wheel   562888 May 16 16:09 graph.pbtxt\r\n",
      "drwxr-xr-x   6 tarrade  wheel      192 May 16 16:09 \u001b[34mkeras\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  wheel  4167824 May 16 16:10 model.ckpt-1000.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  wheel     1118 May 16 16:10 model.ckpt-1000.index\r\n",
      "-rw-r--r--   1 tarrade  wheel   226199 May 16 16:10 model.ckpt-1000.meta\r\n",
      "-rw-r--r--   1 tarrade  wheel  4167824 May 16 16:10 model.ckpt-980.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  wheel     1118 May 16 16:10 model.ckpt-980.index\r\n",
      "-rw-r--r--   1 tarrade  wheel   226199 May 16 16:10 model.ckpt-980.meta\r\n",
      "-rw-r--r--   1 tarrade  wheel  4167824 May 16 16:10 model.ckpt-990.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  wheel     1118 May 16 16:10 model.ckpt-990.index\r\n",
      "-rw-r--r--   1 tarrade  wheel   226199 May 16 16:10 model.ckpt-990.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la /tmp/test2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10f5d42e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard  --logdir   {'/tmp/test2'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0417 13:41:14.449864 4620772800 dataset_builder.py:157] Overwrite dataset info from restored data version.\n",
      "I0417 13:41:14.452956 4620772800 dataset_builder.py:193] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                346176    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_5 (Ba (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 347,402\n",
      "Trainable params: 347,274\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n",
      "train\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 3s 603ms/step - loss: 1.4433 - accuracy: 0.5781\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.7295 - accuracy: 0.8062\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.5221 - accuracy: 0.8656\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4369 - accuracy: 0.8844\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4093 - accuracy: 0.8813\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.2764 - accuracy: 0.9438\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3123 - accuracy: 0.9187\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.2823 - accuracy: 0.9312\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.2870 - accuracy: 0.9031\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.2306 - accuracy: 0.9312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2dbfc438>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "# Define the estimator's input_fn\n",
    "STEPS_PER_EPOCH = 5\n",
    "#BUFFER_SIZE = 10 # Use a much larger value for real code. \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "def input_fn():\n",
    "    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "    mnist_train, mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "    \n",
    "        return image, label[..., tf.newaxis]\n",
    "\n",
    "    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    return train_data.repeat()\n",
    "\n",
    "\n",
    "def make_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "tbCallBack=tf.keras.callbacks.TensorBoard(log_dir='/tmp/test3/',\n",
    "                                          histogram_freq=1,\n",
    "                                          write_graph=True)\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "optimiser=tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, epsilon=1e-07)\n",
    "\n",
    "model.compile(optimizer=optimiser,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "training_dataset=input_fn()\n",
    "\n",
    "print(\"train\")\n",
    "model.fit(training_dataset,\n",
    "          steps_per_epoch=5,\n",
    "          epochs=10,\n",
    "          callbacks=[tbCallBack],\n",
    "          verbose =1)\n",
    "\n",
    "#print(\"evaluate\")\n",
    "#model.evaluate(training_dataset,\n",
    "#              steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard.notebook extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard.notebook\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17713), started -1 day, 23:03:03 ago. (Use '!kill 17713' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10a8cbc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard.notebook\n",
    "%tensorboard  --logdir   {'/tmp/test3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "drwxr-xr-x   5 tarrade  wheel  160 Apr 10 22:03 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxrwxrwt  12 root     wheel  384 Apr 10 22:03 \u001b[30m\u001b[42m..\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   3 tarrade  wheel   96 Apr 10 22:03 \u001b[34mplugins\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   3 tarrade  wheel   96 Apr 10 22:03 \u001b[34mtrain\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   3 tarrade  wheel   96 Apr 10 22:03 \u001b[34mvalidation\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la /tmp/test3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48664\r\n",
      "drwxr-xr-x  14 tarrade  staff      448 Apr 10 21:17 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   4 tarrade  staff      128 Apr 10 21:13 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  staff      177 Apr 10 21:17 checkpoint\r\n",
      "-rw-r--r--   1 tarrade  staff   333317 Apr 10 21:13 graph.pbtxt\r\n",
      "drwxr-xr-x   6 tarrade  staff      192 Apr 10 21:13 \u001b[34mkeras\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  staff  8036500 Apr 10 21:17 model.ckpt-1000.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  staff      889 Apr 10 21:17 model.ckpt-1000.index\r\n",
      "-rw-r--r--   1 tarrade  staff   143942 Apr 10 21:17 model.ckpt-1000.meta\r\n",
      "-rw-r--r--   1 tarrade  staff  8036500 Apr 10 21:17 model.ckpt-980.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  staff      889 Apr 10 21:17 model.ckpt-980.index\r\n",
      "-rw-r--r--   1 tarrade  staff   143942 Apr 10 21:17 model.ckpt-980.meta\r\n",
      "-rw-r--r--   1 tarrade  staff  8036500 Apr 10 21:17 model.ckpt-990.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  staff      889 Apr 10 21:17 model.ckpt-990.index\r\n",
      "-rw-r--r--   1 tarrade  staff   143942 Apr 10 21:17 model.ckpt-990.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la ../../results/Models/Mnist/tf_1_12/estimator/v2/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0408 14:08:27.861193 4771767744 cross_device_ops.py:1111] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy at 0x113427eb8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0408 14:18:37.378367 4771767744 <ipython-input-8-2fd3015b66b4>:4] test\n",
      "W0408 14:18:37.384752 4771767744 cross_device_ops.py:1111] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy at 0xb34700cf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.DEBUG)\n",
    "logging.debug('test')\n",
    "tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# need to be defined if working behind a proxy\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']\n",
    "os.environ['HTTPS_PROXY']\n",
    "os.environ['REQUESTS_CA_BUNDLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['HTTPS_PROXY'] = \"https://proxy-url:Port\"\n",
    "#os.environ['REQUESTS_CA_BUNDLE'] = \"C:/Users/path/to/certs\"\n",
    "#os.environ[\"PROJECT_ID\"] = \"project-id-34914\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project=os.environ['PROJECT_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\mnist\\lib\\site-packages\\google\\auth\\_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "client = storage.Client(project=project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in client.list_buckets():\n",
    "    print(b.name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://googleapis.github.io/google-cloud-python/latest/bigquery/usage/pandas.html\n",
    "https://googleapis.github.io/google-cloud-python/latest/bigquery/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row((2009, 343139, 99.7), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2010, 693332, 99.1), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2011, 1198587, 97.2), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2012, 1642687, 94.6), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2013, 2056613, 91.6), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2014, 2160361, 88.5), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2015, 2214389, 86.4), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT\n",
    "  EXTRACT(YEAR FROM creation_date) AS Year,\n",
    "  COUNT(*) AS Number_of_Questions,\n",
    "  ROUND(100 * SUM(IF(answer_count > 0, 1, 0)) / COUNT(*), 1) AS Percent_Questions_with_Answers\n",
    "FROM\n",
    "  `bigquery-public-data.stackoverflow.posts_questions`\n",
    "GROUP BY\n",
    "  Year\n",
    "HAVING\n",
    "  Year > 2008 AND Year < 2016\n",
    "ORDER BY\n",
    "  Year\n",
    "\"\"\"\n",
    "#df = client.query(query).to_dataframe()\n",
    "#df.head()\n",
    "query_job = client.query(query)\n",
    "rows = query_job.result()  # Waits for query to finish\n",
    "\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:  ('Year', 'Number_of_Questions', 'Percent_Questions_with_Answers')\n",
      "Values:  (2015, 2214389, 86.4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys: \", tuple(row.keys()))\n",
    "print(\"Values: \", row.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Number_of_Questions</th>\n",
       "      <th>Percent_Questions_with_Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009</td>\n",
       "      <td>343139</td>\n",
       "      <td>99.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>693332</td>\n",
       "      <td>99.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>1198587</td>\n",
       "      <td>97.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>1642687</td>\n",
       "      <td>94.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>2056613</td>\n",
       "      <td>91.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2014</td>\n",
       "      <td>2160361</td>\n",
       "      <td>88.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015</td>\n",
       "      <td>2214389</td>\n",
       "      <td>86.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Number_of_Questions  Percent_Questions_with_Answers\n",
       "0  2009               343139                            99.7\n",
       "1  2010               693332                            99.1\n",
       "2  2011              1198587                            97.2\n",
       "3  2012              1642687                            94.6\n",
       "4  2013              2056613                            91.6\n",
       "5  2014              2160361                            88.5\n",
       "6  2015              2214389                            86.4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = client.query(query).to_dataframe()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:env_gcp_dl_2_0_nightly]",
   "language": "python",
   "name": "conda-env-env_gcp_dl_2_0_nightly-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
