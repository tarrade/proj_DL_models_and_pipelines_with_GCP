{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification example with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages on Google  Cloud Datalab (locally use conda env)\n",
    "### Select in the Python3 Kernel:\n",
    "In the menu bar the of 'Kernel', select   \n",
    "**python3**\n",
    "### Install needed packages\n",
    "copy the command below in a Google Cloud Datalab cell  \n",
    "**!pip install tensorflow==1.12**\n",
    "### Restart the Kernel \n",
    "this is to take into account the new installed packages. Click in the menu bar on:  \n",
    "**Reset Session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needed librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gzip\n",
    "import sys\n",
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-preview'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working 2.0: tf.logging -> logging\n",
    "#logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug('test')\n",
    "logging.info('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mnist data, split between train and test sets\n",
    "# on GCP\n",
    "#(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# with AXA network\n",
    "def load_data(path):\n",
    "    f = gzip.open(path, 'rb')\n",
    "    if sys.version_info < (3,):\n",
    "        data = cPickle.load(f)\n",
    "    else:\n",
    "        data = cPickle.load(f, encoding='bytes')\n",
    "    f.close()\n",
    "    return data\n",
    "(x_train, y_train), (x_test, y_test) = load_data(path='../data/mnist.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data shape (training)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data shape (train)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('uint8'), dtype('uint8'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype, x_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 0, 255, 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(x_train), np.min(x_train), np.max(x_test), np.min(x_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.476806640625"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.nbytes/1024.0**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.86083984375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.nbytes/1024.0**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0095367431640625"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.nbytes/1024.0**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057220458984375"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.nbytes/1024.0**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize and reorganize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast uint8 -> float32\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renormalize the data 255 grey variation\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data 28 x 28 -> 784\n",
    "x_train = x_train.reshape(len(x_train), x_train.shape[1]*x_train.shape[2])\n",
    "x_test = x_test.reshape(len(x_test), x_test.shape[1]*x_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train), np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_input=x_train.shape[1]\n",
    "dim_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, label) in enumerate(zip(x_train[0:5], y_train[0:5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Training: %i\\n' % np.argmax(label), fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "learning_rate = 0.5\n",
    "\n",
    "# number of epoch to train our model\n",
    "EPOCHS = 5\n",
    "\n",
    "# size of our mini batch\n",
    "#BATCH_SIZE = len(x_train)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# shuffle buffer size\n",
    "SHUFFLE_BUFFER_SIZE = 10 * BATCH_SIZE\n",
    "\n",
    "# prefetch buffer size\n",
    "PREFETCH_BUFFER_SIZE = 1 #tf.contrib.data.AUTOTUNE\n",
    "\n",
    "# number of paralell calls\n",
    "NUM_PARALELL_CALL = 4\n",
    "\n",
    "# hidden layer 1\n",
    "n1=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE, EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working 2.0: tf.flags -> flags\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "    \n",
    "del_all_flags(flags.FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0/lib/python3.6/site-packages/ipykernel_launcher.py']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flags.DEFINE_string('f', '', 'kernel') # just for jupyter notebook and avoir : \"UnrecognizedFlagError: Unknown command line flag 'f'\"\n",
    "flags.DEFINE_string('model_dir', '../results/Models/Mnist/ckpt/', 'Dir to save a model and checkpoints')\n",
    "flags.DEFINE_string('saved_dir', '../results/Models/Mnist/pb/', 'Dir to save a model for TF serving')\n",
    "flags.DEFINE_integer('shuffle_buffer_size', SHUFFLE_BUFFER_SIZE , 'Shuffle buffer size')\n",
    "flags.DEFINE_integer('prefetch_buffer_size', PREFETCH_BUFFER_SIZE, 'Prefetch buffer size')\n",
    "flags.DEFINE_integer('batch_size', BATCH_SIZE, 'Batch size')\n",
    "flags.DEFINE_integer('epoch', EPOCHS, 'number of epoch')\n",
    "flags.DEFINE_integer('num_parallel_calls', NUM_PARALELL_CALL, 'Number of paralell calls')\n",
    "FLAGS = flags.FLAGS\n",
    "# new with 2.0\n",
    "FLAGS(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0/lib/python3.6/site-packages/ipykernel_launcher.py:\n",
      "  --batch_size: Batch size\n",
      "    (default: '128')\n",
      "    (an integer)\n",
      "  --epoch: number of epoch\n",
      "    (default: '5')\n",
      "    (an integer)\n",
      "  --f: kernel\n",
      "    (default: '')\n",
      "  --model_dir: Dir to save a model and checkpoints\n",
      "    (default: '../results/Models/Mnist/ckpt/')\n",
      "  --num_parallel_calls: Number of paralell calls\n",
      "    (default: '4')\n",
      "    (an integer)\n",
      "  --prefetch_buffer_size: Prefetch buffer size\n",
      "    (default: '1')\n",
      "    (an integer)\n",
      "  --saved_dir: Dir to save a model for TF serving\n",
      "    (default: '../results/Models/Mnist/pb/')\n",
      "  --shuffle_buffer_size: Shuffle buffer size\n",
      "    (default: '1280')\n",
      "    (an integer)\n",
      "\n",
      "absl.flags:\n",
      "  --flagfile: Insert flag definitions from the given file into the command line.\n",
      "    (default: '')\n",
      "  --undefok: comma-separated list of flag names that it is okay to specify on\n",
      "    the command line even if the program does not define a flag with that name.\n",
      "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\n",
      "    format.\n",
      "    (default: '')\n"
     ]
    }
   ],
   "source": [
    "print(FLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf.data.Dataset\n",
    "https://www.tensorflow.org/guide/performance/datasets  \n",
    "To summarize, one good order for the different transformations is:\n",
    "- create the dataset\n",
    "- shuffle (with a big enough buffer size https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle)\n",
    "- repeat\n",
    "- map with the actual work (preprocessing, augmentationâ€¦) using multiple parallel calls\n",
    "- batch\n",
    "- prefetch\n",
    "\n",
    "ModeKeys:  \n",
    "https://www.tensorflow.org/api_docs/python/tf/estimator/ModeKeys  \n",
    "- EVAL\n",
    "- PREDICT\n",
    "- TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test', 'predict', 'train')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.estimator.ModeKeys.EVAL, tf.estimator.ModeKeys.PREDICT, tf.estimator.ModeKeys.TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_dataset_fn(x_data, y_data, batch_size=128, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        logging.info(\"input_dataset_fn: PREDICT, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        logging.info(\"input_dataset_fn: EVAL, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        logging.info(\"input_dataset_fn: TRAIN, {}\".format(mode))\n",
    "    \n",
    "    # 1) convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "    \n",
    "    # 2) shuffle (with a big enough buffer size)    :        \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None # loop indefinitely\n",
    "        dataset = dataset.shuffle(buffer_size=FLAGS.shuffle_buffer_size, seed=2)# depends on sample size\n",
    "    else:\n",
    "        num_epochs = 1 # end-of-input after this\n",
    "        \n",
    "    print('the number of epoch: num_epoch =', num_epochs)\n",
    "        \n",
    "    # caching data\n",
    "    #dataset = dataset.cache()\n",
    "    \n",
    "    # 3) automatically refill the data queue when empty\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    \n",
    "    # 4) map\n",
    "    #dataset = dataset.map(map_func=parse_fn, num_parallel_calls=FLAGS.num_parallel_calls)\n",
    "\n",
    "    # 5) create batches of data\n",
    "    dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "    \n",
    "    # 6) prefetch data for faster consumption, based on your system and environment, allows the tf.data runtime to automatically tune the prefetch buffer sizes\n",
    "    dataset = dataset.prefetch(FLAGS.prefetch_buffer_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of epoch: num_epoch = None\n",
      "the number of epoch: num_epoch = 1\n"
     ]
    }
   ],
   "source": [
    "training_dataset = input_dataset_fn(x_train, \n",
    "                                    y_train, \n",
    "                                    mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                                    batch_size=FLAGS.batch_size)\n",
    "testing_dataset = input_dataset_fn(x_test, \n",
    "                                   y_test,\n",
    "                                   mode=tf.estimator.ModeKeys.EVAL, \n",
    "                                   batch_size=len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration n: 0 execution time: 0.00012699999999199463 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 3 \n",
      "\n",
      "iteration n: 1 execution time: 1.2999999995599865e-05 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 8 \n",
      "\n",
      "iteration n: 2 execution time: 1.3999999993075107e-05 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 8 \n",
      "\n",
      "iteration n: 3 execution time: 1.1999999998124622e-05 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 2 \n",
      "\n",
      "iteration n: 4 execution time: 1.3999999993075107e-05 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 6 \n",
      "\n",
      "iteration n: 5 execution time: 7.999999999697138e-05 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 3 \n",
      "\n",
      "iteration n: 6 execution time: 9.999999974752427e-06 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 4 \n",
      "\n",
      "iteration n: 7 execution time: 1.100000000064938e-05 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 8 execution time: 1.100000000064938e-05 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 9 execution time: 1.2999999995599865e-05 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 8 \n",
      "\n",
      "number of iteration reached\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "n_iter=10 #len(x_train)//BATCH_SIZE\n",
    "for features, labels in training_dataset:\n",
    "    try:\n",
    "        start_time = time.clock()\n",
    "        x,y = features, labels\n",
    "        print('iteration n:', n, 'execution time:', time.clock() - start_time, 'seconds')\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "        n+=1\n",
    "        if n>=n_iter:\n",
    "            print('number of iteration reached')\n",
    "            break\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('tf.errors.OutOfRangeError')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001998000000014599 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "number of iteration reached\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "n_iter=1\n",
    "for features, labels in testing_dataset:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = features, labels\n",
    "            print(time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "            n+=1\n",
    "            if n>=n_iter:\n",
    "                print('number of iteration reached')\n",
    "                break\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our model\n",
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = tf.keras.Sequential()\n",
    " \n",
    "    # hidden layer\n",
    "    model.add(tf.keras.layers.Dense(dim_input, \n",
    "                    input_dim=dim_input, \n",
    "                    kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                    bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                    activation='relu'))\n",
    "    # last layer\n",
    "    model.add(tf.keras.layers.Dense(num_classes, \n",
    "                    kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                    bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                    activation='softmax'))\n",
    "    \n",
    "    # weight initialisation\n",
    "    # He: keras.initializers.he_normal(seed=None)\n",
    "    # Xavier: keras.initializers.glorot_uniform(seed=None)\n",
    "    # Radom Normal: keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "    # Truncated Normal: keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "    \n",
    "    # optimiser (use tf.train and not tf.keras to use MirrorStrategy)\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/train/Optimizer\n",
    "    #optimiser=tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9)\n",
    "    # GD/SGC:   tf.train.GradientDescentOptimizer(learning_rate, use_locking=False, name='GradientDescent') \n",
    "    # Adam:     tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False,name='Adam')\n",
    "    # RMSProp:  tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.0, epsilon=1e-10, use_locking=False, centered=False, name='RMSProp')\n",
    "    # Momentum: tf.train.MomentumOptimizer(learning_rate, momentum, use_locking=False, name='Momentum', use_nesterov=False)\n",
    "\n",
    "   \n",
    "    optimiser=tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9)\n",
    "    # GD/SGC:   keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    # Adam:     keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    # RMSProp:  keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    # Momentum: keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimiser, \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the model\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the nuber of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 623,290\n",
      "Trainable params: 623,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check input and output layer names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_input']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_names # Use this name as the dictionary key in the TF input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_1']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Call back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print info during iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UDFPrint(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        _, __, x_train, y_train = self.test_data\n",
    "        loss_train, acc_train = self.model.evaluate(x_train, y_train, verbose=0)\n",
    "        print('Reached epoch {0:3d} cost J = {1:.5f}'.format(epoch, loss_train))\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x_test, y_test, x_train, y_train = self.test_data\n",
    "        loss_train, acc_train = self.model.evaluate(x_train, y_train, verbose=0)\n",
    "        loss_test, acc_test = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        print(' accurary on the training set {0:.4f}'.format(acc_train))\n",
    "        print(' accurary on the testing set {0:.4f}'.format(acc_test))\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        #print('  ---> starting minibatch', batch)\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        #print('  ---> ending minibatch', batch)\n",
    "        return\n",
    "return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "#tbCallBack=tf.keras.callbacks.TensorBoard(log_dir='../results/TensorBoard/Mnist/logs/', \n",
    "#                                          histogram_freq=1, \n",
    "#                                          write_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model using Keras and numpy array  \n",
    "- batch_size \n",
    "  determines the number of samples in each mini batch. Its maximum is the number of all samples, which makes gradient descent accurate, the loss will decrease towards the minimum if the learning rate is small enough, but iterations are slower. Its minimum is 1, resulting in stochastic gradient descent: Fast but the direction of the gradient step is based only on one example, the loss may jump around. batch_size allows to adjust between the two extremes: accurate gradient direction and fast iteration. Also, the maximum value for batch_size may be limited if your model + data set does not fit into the available (GPU) memory.\n",
    "- steps_per_epoch \n",
    "  the number of batch iterations before a training epoch is considered finished. If you have a training set of fixed size you can ignore it but it may be useful if you have a huge data set or if you are generating random data augmentations on the fly, i.e. if your training set has a (generated) infinite size. If you have the time to go through your whole training data set I recommend to skip this parameter.\n",
    "- validation_steps \n",
    "  similar to steps_per_epoch but on the validation data set instead on the training data. If you have the time to go through your whole validation data set I recommend to skip this parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 - 3s - loss: 0.2172 - acc: 0.9370 - val_loss: 0.1217 - val_acc: 0.9618\n",
      "Epoch 2/10\n",
      "60000/60000 - 2s - loss: 0.1092 - acc: 0.9670 - val_loss: 0.1298 - val_acc: 0.9608\n",
      "Epoch 3/10\n",
      "60000/60000 - 2s - loss: 0.0918 - acc: 0.9732 - val_loss: 0.1289 - val_acc: 0.9668\n",
      "Epoch 4/10\n",
      "60000/60000 - 2s - loss: 0.0778 - acc: 0.9772 - val_loss: 0.1190 - val_acc: 0.9717\n",
      "Epoch 5/10\n",
      "60000/60000 - 2s - loss: 0.0732 - acc: 0.9797 - val_loss: 0.1422 - val_acc: 0.9691\n",
      "Epoch 6/10\n",
      "60000/60000 - 3s - loss: 0.0697 - acc: 0.9806 - val_loss: 0.1328 - val_acc: 0.9714\n",
      "Epoch 7/10\n",
      "60000/60000 - 2s - loss: 0.0625 - acc: 0.9831 - val_loss: 0.1598 - val_acc: 0.9714\n",
      "Epoch 8/10\n",
      "60000/60000 - 2s - loss: 0.0593 - acc: 0.9847 - val_loss: 0.1881 - val_acc: 0.9689\n",
      "Epoch 9/10\n",
      "60000/60000 - 2s - loss: 0.0534 - acc: 0.9861 - val_loss: 0.1509 - val_acc: 0.9742\n",
      "Epoch 10/10\n",
      "60000/60000 - 2s - loss: 0.0508 - acc: 0.9876 - val_loss: 0.2041 - val_acc: 0.9706\n"
     ]
    }
   ],
   "source": [
    "# Set to the original weights for testing other pipelines\n",
    "model.set_weights(initial_weights)\n",
    "\n",
    "# Fit the model\n",
    "hist=model.fit(x_train, \n",
    "               y_train, \n",
    "               validation_data=(x_test, y_test),\n",
    "               #callbacks=[UDFPrint((x_test, y_test, x_train, y_train)), tbCallBack],\n",
    "               epochs=EPOCHS, \n",
    "               batch_size=BATCH_SIZE,\n",
    "               verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.21717953401803972, 0.10917758003473282, 0.09176977903321386, 0.07778290131042401, 0.07319211648752293, 0.06971756339594722, 0.06250256696790457, 0.05933427373357117, 0.05336977696067964, 0.0507808894061173], 'acc': [0.93701667, 0.9670333, 0.97325, 0.97718334, 0.97966665, 0.9805667, 0.9830833, 0.98468333, 0.98611665, 0.9876], 'val_loss': [0.12166773548126221, 0.12982718848884106, 0.12888396408557892, 0.1189963302373886, 0.14218190791606902, 0.13277554471120237, 0.15981805674433708, 0.18809437324916944, 0.15093745285563637, 0.20406991792917228], 'val_acc': [0.9618, 0.9608, 0.9668, 0.9717, 0.9691, 0.9714, 0.9714, 0.9689, 0.9742, 0.9706]}\n"
     ]
    }
   ],
   "source": [
    "print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.20406991853305018\n",
      "Test accuracy: 0.9706\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, \n",
    "                       y_test, \n",
    "                       verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.05631555794287784\n",
      "Train accuracy: 0.9859833\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_train, \n",
    "                       y_train, \n",
    "                       verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with keras optimiser we save save the model+weight\n",
    "model.save('../results/Models/Mnist/keras_model.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start in a separte shell with the env activated:\n",
    "tensorboard --logdir your_path/proj_DL_models_and_pipelines_with_GCP/results/TensorBoard/Mnist/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model using Keras annd tf.data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of epoch: num_epoch = None\n",
      "the number of epoch: num_epoch = 1\n",
      "Epoch 1/10\n",
      "468/468 - 2s - loss: 0.3445 - acc: 0.9226 - val_loss: 0.1559 - val_acc: 0.9552\n",
      "Epoch 2/10\n",
      "468/468 - 2s - loss: 0.1222 - acc: 0.9639 - val_loss: 0.1227 - val_acc: 0.9655\n",
      "Epoch 3/10\n",
      "468/468 - 2s - loss: 0.0868 - acc: 0.9731 - val_loss: 0.1361 - val_acc: 0.9651\n",
      "Epoch 4/10\n",
      "468/468 - 2s - loss: 0.0731 - acc: 0.9771 - val_loss: 0.1484 - val_acc: 0.9649\n",
      "Epoch 5/10\n",
      "468/468 - 2s - loss: 0.0657 - acc: 0.9798 - val_loss: 0.1431 - val_acc: 0.9660\n",
      "Epoch 6/10\n",
      "468/468 - 2s - loss: 0.0582 - acc: 0.9816 - val_loss: 0.1509 - val_acc: 0.9646\n",
      "Epoch 7/10\n",
      "468/468 - 2s - loss: 0.0535 - acc: 0.9830 - val_loss: 0.1408 - val_acc: 0.9683\n",
      "Epoch 8/10\n",
      "468/468 - 2s - loss: 0.0536 - acc: 0.9838 - val_loss: 0.1608 - val_acc: 0.9659\n",
      "Epoch 9/10\n",
      "468/468 - 2s - loss: 0.0463 - acc: 0.9858 - val_loss: 0.1609 - val_acc: 0.9666\n",
      "Epoch 10/10\n",
      "468/468 - 2s - loss: 0.0424 - acc: 0.9868 - val_loss: 0.1566 - val_acc: 0.9688\n"
     ]
    }
   ],
   "source": [
    "# Set to the original weights for testing other pipelines\n",
    "model.set_weights(initial_weights)\n",
    "\n",
    "# Fit the model (using data.Dataset)\n",
    "hist=model.fit(input_dataset_fn(x_train, y_train, mode=tf.estimator.ModeKeys.TRAIN, batch_size=FLAGS.batch_size),\n",
    "               steps_per_epoch=len(x_train) // BATCH_SIZE,\n",
    "               validation_data=input_dataset_fn(x_test, y_test, mode=tf.estimator.ModeKeys.EVAL, batch_size=len(x_test)),\n",
    "               validation_steps=1, \n",
    "               #callbacks=[UDFPrint((x_test, y_test, x_train, y_train)), tbCallBack],\n",
    "               epochs=EPOCHS,\n",
    "               verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.34453886528857625, 0.12224854580445103, 0.0868404434991873, 0.07308664929686895, 0.06566301603357777, 0.05817660538312525, 0.05350063713794797, 0.05360552344635889, 0.04634311653714195, 0.04242700067984088], 'acc': [0.9226262, 0.9639423, 0.97314036, 0.9770967, 0.97975093, 0.9816206, 0.9830061, 0.9837573, 0.98579395, 0.98679554], 'val_loss': [0.1559320092201233, 0.12271258234977722, 0.13613088428974152, 0.14838959276676178, 0.1431269347667694, 0.1509007066488266, 0.14084689319133759, 0.16083888709545135, 0.1609259396791458, 0.15659475326538086], 'val_acc': [0.9552, 0.9655, 0.9651, 0.9649, 0.966, 0.9646, 0.9683, 0.9659, 0.9666, 0.9688]}\n"
     ]
    }
   ],
   "source": [
    "print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.1565948262046947\n",
      "Test accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, \n",
    "                       y_test, \n",
    "                       verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.041119411910375374\n",
      "Train accuracy: 0.98751664\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_train, \n",
    "                       y_train, \n",
    "                       verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start in a separte shell with the env activated:\n",
    "tensorboard --logdir your_path/proj_DL_models_and_pipelines_with_GCP/results/TensorBoard/Mnist/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model using tf.estimator and tf.data.dataset\n",
    "### Create some helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'SessionRunHook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-5bf49fec0688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTimeHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSessionRunHook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbefore_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'SessionRunHook'"
     ]
    }
   ],
   "source": [
    "class TimeHistory(tf.train.SessionRunHook):\n",
    "    def begin(self):\n",
    "        self.times = []\n",
    "\n",
    "    def before_run(self, run_context):\n",
    "        self.iter_time_start = time.time()\n",
    "\n",
    "    def after_run(self, run_context, run_values):\n",
    "        self.times.append(time.time() - self.iter_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_hist = TimeHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf.distribute.startegy work across multiple devices/machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-d81e6fd8f68a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#strategy=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## doesn't work with Keras ?!?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOneDeviceStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device:CPU:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#strategy = tf.contrib.distribute.OneDeviceStrategy('device:GPU:0')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "# the tf.distribute.Strategy API is an easy way to distribute your training across multiple devices/machines\n",
    "NUM_GPUS = 2\n",
    "#strategy=None\n",
    "## doesn't work with Keras ?!?\n",
    "strategy = tf.contrib.distribute.OneDeviceStrategy('device:CPU:0')\n",
    "#strategy = tf.contrib.distribute.OneDeviceStrategy('device:GPU:0')\n",
    "#strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS)\n",
    "#strategy = tf.contrib.distribute.MirroredStrategy()\n",
    "\n",
    "# config tf.estimator to use a give strategy\n",
    "training_config = tf.estimator.RunConfig(train_distribute=strategy,\n",
    "                                         model_dir=FLAGS.model_dir,\n",
    "                                         save_summary_steps=20,\n",
    "                                         save_checkpoints_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transfor keras model to estimator model\n",
    "delete fist the folder for a clean start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r ../results/Models/Mnist/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rmdir /S /Q \"../results/Models/Mnist/ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-409dcf00b57d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set to the original weights for testing other pipelines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# transfor keras model to estimator model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m estimator_train_model = tf.keras.estimator.model_to_estimator(keras_model=model,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Set to the original weights for testing other pipelines\n",
    "model.set_weights(initial_weights)\n",
    "\n",
    "# transfor keras model to estimator model\n",
    "estimator_train_model = tf.keras.estimator.model_to_estimator(keras_model=model,\n",
    "                                                              config=training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mkeras\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../results/Models/Mnist/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir /B \"../results/Models/Mnist/ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train(\n",
    "    input_fn,\n",
    "    hooks=None,\n",
    "    steps=None,\n",
    "    max_steps=None,\n",
    "    saving_listeners=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_input_fn():\n",
    "    return input_dataset_fn(x_train,\n",
    "                            y_train, \n",
    "                            mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                            batch_size=FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'estimator_train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b1ffdb5cc260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#                            max_steps=1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#                            hooks=[time_hist])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m estimator_train_model.train(input_fn=get_train_input_fn,\n\u001b[0m\u001b[1;32m      6\u001b[0m                             \u001b[0;31m#max_steps=1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                             steps=1000)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'estimator_train_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit the model (using estimator.train and data.Dataset)\n",
    "#estimator_train_model.train(input_fn=lambda:input_dataset_fn(x_train, y_train, is_training=True, batch_size=BATCH_SIZE),\n",
    "#                            max_steps=1,\n",
    "#                            hooks=[time_hist])\n",
    "estimator_train_model.train(input_fn=get_train_input_fn,\n",
    "                            #max_steps=1,\n",
    "                            steps=1000)\n",
    "                            #hooks=[time_hist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time with the current strategy: 164.70961594581604 seconds\n"
     ]
    }
   ],
   "source": [
    "total_time = sum(time_hist.times)\n",
    "print(f\"total time with the current strategy: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777.1252410794748 images/second\n"
     ]
    }
   ],
   "source": [
    "avg_time_per_batch = np.mean(time_hist.times)\n",
    "print(f\"{BATCH_SIZE/avg_time_per_batch} images/second\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start in a separte shell with the env activated:\n",
    "tensorboard --logdir your_path/proj_DL_models_and_pipelines_with_GCP/results/Models/Mnist/ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the accuracy of our model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "evaluate(\n",
    "    input_fn,\n",
    "    steps=None,\n",
    "    hooks=None,\n",
    "    checkpoint_path=None,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: TRAIN, train\n",
      "the number of epoch: num_epoch = None\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-24-19:08:19\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../results/Models/Mnist/ckpt/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-24-19:08:36\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.98143333, global_step = 1000, loss = 0.06319532\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: ../results/Models/Mnist/ckpt/model.ckpt-1000\n"
     ]
    }
   ],
   "source": [
    "score=estimator_train_model.evaluate(input_fn=lambda:input_dataset_fn(x_train,y_train, mode=tf.estimator.ModeKeys.TRAIN, batch_size=len(y_train)),\n",
    "                                    steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.98143333, 'loss': 0.06319532, 'global_step': 1000}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.06319532\n",
      "Train accuracy: 0.98143333\n",
      "Train global steps: 1000\n"
     ]
    }
   ],
   "source": [
    "print('Train loss:', score['loss'])\n",
    "print('Train accuracy:', score['accuracy'])\n",
    "print('Train global steps:', score['global_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: EVAL, eval\n",
      "the number of epoch: num_epoch = 1\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-24-14:00:20\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../results/Models/Mnist/ckpt/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-24-14:00:21\n",
      "INFO:tensorflow:Saving dict for global step 10: accuracy = 0.808, global_step = 10, loss = 0.77520794\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10: ../results/Models/Mnist/ckpt/model.ckpt-10\n"
     ]
    }
   ],
   "source": [
    "score=estimator_train_model.evaluate(input_fn=lambda:input_dataset_fn(x_test, y_test,mode=tf.estimator.ModeKeys.EVAL, batch_size=len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.77520794\n",
      "Test accuracy: 0.808\n",
      "Test global steps: 10\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', score['loss'])\n",
    "print('Test accuracy:', score['accuracy'])\n",
    "print('Test global steps:', score['global_step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating predictions on our trained model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predict(\n",
    "    input_fn,\n",
    "    predict_keys=None,\n",
    "    hooks=None,\n",
    "    checkpoint_path=None,\n",
    "    yield_single_examples=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: EVAL, eval\n",
      "the number of epoch: num_epoch = 1\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../results/Models/Mnist/ckpt/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predictions=list(estimator_train_model.predict(input_fn=lambda:input_dataset_fn(x_test, y_test,mode=tf.estimator.ModeKeys.EVAL, batch_size=len(x_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dense_1'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_input']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_1']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense/bias',\n",
       " 'dense/bias/Adam',\n",
       " 'dense/bias/Adam_1',\n",
       " 'dense/kernel',\n",
       " 'dense/kernel/Adam',\n",
       " 'dense/kernel/Adam_1',\n",
       " 'dense_1/bias',\n",
       " 'dense_1/bias/Adam',\n",
       " 'dense_1/bias/Adam_1',\n",
       " 'dense_1/kernel',\n",
       " 'dense_1/kernel/Adam',\n",
       " 'dense_1/kernel/Adam_1',\n",
       " 'global_step',\n",
       " 'training/TFOptimizer/beta1_power',\n",
       " 'training/TFOptimizer/beta2_power']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_train_model.get_variable_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 7\n",
      "Predicted label:  7\n",
      "Actual label: 2\n",
      "Predicted label:  2\n",
      "Actual label: 1\n",
      "Predicted label:  1\n",
      "Actual label: 0\n",
      "Predicted label:  0\n",
      "Actual label: 4\n",
      "Predicted label:  4\n",
      "Actual label: 1\n",
      "Predicted label:  1\n",
      "Actual label: 4\n",
      "Predicted label:  4\n",
      "Actual label: 9\n",
      "Predicted label:  9\n",
      "Actual label: 5\n",
      "Predicted label:  4\n",
      "Actual label: 9\n",
      "Predicted label:  9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    prediction_array = predictions[i]['dense_1']\n",
    "    predicted_label = np.argmax(prediction_array)\n",
    "    print('Actual label:', np.argmax(y_test[i]))\n",
    "    print(\"Predicted label: \", predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_train_model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense/bias',\n",
       " 'dense/bias/Adam',\n",
       " 'dense/bias/Adam_1',\n",
       " 'dense/kernel',\n",
       " 'dense/kernel/Adam',\n",
       " 'dense/kernel/Adam_1',\n",
       " 'dense_1/bias',\n",
       " 'dense_1/bias/Adam',\n",
       " 'dense_1/bias/Adam_1',\n",
       " 'dense_1/kernel',\n",
       " 'dense_1/kernel/Adam',\n",
       " 'dense_1/kernel/Adam_1',\n",
       " 'global_step',\n",
       " 'training/TFOptimizer/beta1_power',\n",
       " 'training/TFOptimizer/beta2_power']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_train_model.get_variable_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    \"\"\"Serving input_fn that builds features from placeholders#\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.estimator.export.ServingInputReceiver\n",
    "    \"\"\"\n",
    "    input_images = tf.placeholder(tf.float32, [None, 784])\n",
    "    features = {'dense_input' : input_images} # this is the dict that is then passed as \"features\" parameter to your model_fn\n",
    "    receiver_tensors = {'dense_input': input_images} # As far as I understand this is needed to map the input to a name you can retrieve later\n",
    "   \n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from ../results/Models/Mnist/ckpt/model.ckpt-10\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ../results/Models/Mnist/pb/temp-b'1548343514'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'../results/Models/Mnist/pb/1548343514'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_train_model.export_saved_model(FLAGS.saved_dir, \n",
    "                                         serving_input_receiver_fn=serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/Models/Mnist/pb/1548343514/saved_model.pb\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../results/Models/Mnist/pb/1548343514/saved_model.pb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\r\n",
      "The given SavedModel SignatureDef contains the following output(s):\r\n",
      "Method name is: \r\n"
     ]
    }
   ],
   "source": [
    "! saved_model_cli show --dir ../results/Models/Mnist/pb/1548343514/ --tag serve --signature_def predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"<string>\", line 1, in <module>\r\n",
      "AttributeError: module 'tensorflow' has no attribute 'GIT_VERSION'\r\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /Users/tarrade/anaconda3/envs/env_gcp_dl_2_0:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "absl-py                   0.7.0                     <pip>\r\n",
      "appnope                   0.1.0            py36hf537a9a_0  \r\n",
      "asn1crypto                0.24.0                   py36_0  \r\n",
      "astor                     0.7.1                     <pip>\r\n",
      "astroid                   2.1.0                    py36_0  \r\n",
      "atomicwrites              1.2.1                    py36_0  \r\n",
      "attrs                     18.2.0           py36h28b3542_0  \r\n",
      "autopep8                  1.4.3                    py36_0  \r\n",
      "backcall                  0.1.0                    py36_0  \r\n",
      "blas                      1.0                         mkl  \r\n",
      "bleach                    3.1.0                    py36_0  \r\n",
      "ca-certificates           2018.12.5                     0  \r\n",
      "certifi                   2018.11.29               py36_0  \r\n",
      "cffi                      1.11.5           py36h6174b99_1  \r\n",
      "chardet                   3.0.4                    py36_1  \r\n",
      "colorama                  0.4.1                    py36_0  \r\n",
      "cryptography              2.3.1            py36hdbc3d79_0  \r\n",
      "cycler                    0.10.0           py36hfc81398_0  \r\n",
      "dbus                      1.13.6               h90a0687_0  \r\n",
      "decorator                 4.3.0                    py36_0  \r\n",
      "defusedxml                0.5.0                    py36_1  \r\n",
      "entrypoints               0.3                      py36_0  \r\n",
      "expat                     2.2.6                h0a44026_0  \r\n",
      "freetype                  2.9.1                hb4e5f40_0  \r\n",
      "gast                      0.2.2                     <pip>\r\n",
      "gettext                   0.19.8.1             h15daf44_3  \r\n",
      "gitdb2                    2.0.5                    py36_0  \r\n",
      "gitpython                 2.1.11                   py36_0  \r\n",
      "glib                      2.56.2               hd9629dc_0  \r\n",
      "google-pasta              0.1.1                     <pip>\r\n",
      "grpcio                    1.18.0                    <pip>\r\n",
      "h5py                      2.9.0                     <pip>\r\n",
      "icu                       58.2                 h4b95b61_1  \r\n",
      "idna                      2.8                      py36_0  \r\n",
      "intel-openmp              2019.1                      144  \r\n",
      "ipykernel                 4.10.0                   py36_0  \r\n",
      "ipython                   7.2.0            py36h39e3cac_0  \r\n",
      "ipython_genutils          0.2.0            py36h241746c_0  \r\n",
      "ipywidgets                7.4.1                    py36_0  \r\n",
      "isort                     4.3.4                    py36_0  \r\n",
      "jedi                      0.13.2                   py36_0  \r\n",
      "jinja2                    2.10                     py36_0  \r\n",
      "jpeg                      9b                   he5867d9_2  \r\n",
      "jsonschema                2.6.0            py36hb385e00_0  \r\n",
      "jupyter                   1.0.0                    py36_7  \r\n",
      "jupyter_client            5.2.4                    py36_0  \r\n",
      "jupyter_console           6.0.0                    py36_0  \r\n",
      "jupyter_contrib_core      0.3.3                      py_2    conda-forge\r\n",
      "jupyter_contrib_nbextensions 0.5.0                 py36_1000    conda-forge\r\n",
      "jupyter_core              4.4.0                    py36_0  \r\n",
      "jupyter_highlight_selected_word 0.2.0                 py36_1000    conda-forge\r\n",
      "jupyter_latex_envs        1.4.4                 py36_1000    conda-forge\r\n",
      "jupyter_nbextensions_configurator 0.4.0                 py36_1000    conda-forge\r\n",
      "jupyterlab                0.34.9                   py36_0  \r\n",
      "jupyterlab_launcher       0.13.1                   py36_0  \r\n",
      "Keras-Applications        1.0.6                     <pip>\r\n",
      "Keras-Preprocessing       1.0.5                     <pip>\r\n",
      "kiwisolver                1.0.1            py36h0a44026_0  \r\n",
      "lazy-object-proxy         1.3.1            py36h1de35cc_2  \r\n",
      "libcxx                    4.0.1                hcfea43d_1  \r\n",
      "libcxxabi                 4.0.1                hcfea43d_1  \r\n",
      "libedit                   3.1.20181209         hb402a30_0  \r\n",
      "libffi                    3.2.1                h475c297_4  \r\n",
      "libgfortran               3.0.1                h93005f0_2  \r\n",
      "libiconv                  1.15                 hdd342a3_7  \r\n",
      "libpng                    1.6.36               ha441bb4_0  \r\n",
      "libsodium                 1.0.16               h3efe00b_0  \r\n",
      "libxml2                   2.9.9                hab757c2_0  \r\n",
      "libxslt                   1.1.33               h33a18ac_0  \r\n",
      "lxml                      4.3.0            py36hef8c89e_0  \r\n",
      "Markdown                  3.0.1                     <pip>\r\n",
      "markupsafe                1.1.0            py36h1de35cc_0  \r\n",
      "matplotlib                3.0.2            py36h54f8f79_0  \r\n",
      "mccabe                    0.6.1                    py36_1  \r\n",
      "mistune                   0.8.4            py36h1de35cc_0  \r\n",
      "mkl                       2018.0.3                      1  \r\n",
      "mkl_fft                   1.0.6            py36hb8a8100_0  \r\n",
      "mkl_random                1.0.1            py36h5d10147_1  \r\n",
      "more-itertools            5.0.0                    py36_0  \r\n",
      "nbconvert                 5.4.0                    py36_1  \r\n",
      "nbdime                    1.0.4                 py36_1000    conda-forge\r\n",
      "nbformat                  4.4.0            py36h827af21_0  \r\n",
      "ncurses                   6.1                  h0a44026_1  \r\n",
      "notebook                  5.6.0                    py36_0  \r\n",
      "numpy                     1.14.5           py36h648b28d_4  \r\n",
      "numpy-base                1.14.5           py36ha9ae307_4  \r\n",
      "openssl                   1.0.2p               h1de35cc_0  \r\n",
      "pandas                    0.23.4           py36h6440ff4_0  \r\n",
      "pandoc                    1.19.2.1             ha5e8f32_1  \r\n",
      "pandocfilters             1.4.2                    py36_1  \r\n",
      "parso                     0.3.1                    py36_0  \r\n",
      "pcre                      8.42                 h378b8a2_0  \r\n",
      "pep8                      1.7.1                    py36_0  \r\n",
      "pexpect                   4.6.0                    py36_0  \r\n",
      "pickleshare               0.7.5                    py36_0  \r\n",
      "pip                       18.0                  py36_1001    conda-forge\r\n",
      "pluggy                    0.6.0                    py36_0  \r\n",
      "prometheus_client         0.5.0                    py36_0  \r\n",
      "prompt_toolkit            2.0.7                    py36_0  \r\n",
      "protobuf                  3.6.1                     <pip>\r\n",
      "ptyprocess                0.6.0                    py36_0  \r\n",
      "py                        1.7.0                    py36_0  \r\n",
      "pycodestyle               2.4.0                    py36_0  \r\n",
      "pycparser                 2.19                     py36_0  \r\n",
      "pydocstyle                3.0.0                    py36_0  \r\n",
      "pyflakes                  2.0.0                    py36_0  \r\n",
      "pygments                  2.3.1                    py36_0  \r\n",
      "pylama                    7.4.3                      py_0    conda-forge\r\n",
      "pylint                    2.2.2                    py36_0  \r\n",
      "pyopenssl                 18.0.0                   py36_0  \r\n",
      "pyparsing                 2.3.1                    py36_0  \r\n",
      "pyqt                      5.9.2            py36h655552a_2  \r\n",
      "pysocks                   1.6.8                    py36_0  \r\n",
      "pytest                    3.6.2                    py36_0  \r\n",
      "python                    3.6.6                hc167b69_0  \r\n",
      "python-dateutil           2.7.5                    py36_0  \r\n",
      "pytz                      2018.9                   py36_0  \r\n",
      "pyyaml                    3.13             py36h1de35cc_0  \r\n",
      "pyzmq                     17.1.2           py36h1de35cc_0  \r\n",
      "qt                        5.9.7                h468cd18_1  \r\n",
      "qtconsole                 4.4.3                    py36_0  \r\n",
      "readline                  7.0                  h1de35cc_5  \r\n",
      "requests                  2.21.0                   py36_0  \r\n",
      "scikit-learn              0.20.1           py36h4f467ca_0  \r\n",
      "scipy                     1.1.0            py36hf1f7d93_0  \r\n",
      "send2trash                1.5.0                    py36_0  \r\n",
      "setuptools                40.6.3                   py36_0  \r\n",
      "sip                       4.19.8           py36h0a44026_0  \r\n",
      "six                       1.12.0                   py36_0  \r\n",
      "smmap2                    2.0.5                    py36_0  \r\n",
      "snowballstemmer           1.2.1            py36h6c7b616_0  \r\n",
      "sqlite                    3.26.0               ha441bb4_0  \r\n",
      "tb-nightly                1.13.0a20190125           <pip>\r\n",
      "tensorflow-estimator-2.0-preview 1.13.0.dev2019012500           <pip>\r\n",
      "termcolor                 1.1.0                     <pip>\r\n",
      "terminado                 0.8.1                    py36_1  \r\n",
      "testpath                  0.4.2                    py36_0  \r\n",
      "tf-nightly-2.0-preview    2.0.0rc0                  <pip>\r\n",
      "tk                        8.6.8                ha441bb4_0  \r\n",
      "tornado                   4.5.3                    py36_0  \r\n",
      "traitlets                 4.3.2            py36h65bd3ce_0  \r\n",
      "typed-ast                 1.1.0            py36h1de35cc_0  \r\n",
      "urllib3                   1.24.1                   py36_0  \r\n",
      "watermark                 1.8.0                      py_0    conda-forge\r\n",
      "wcwidth                   0.1.7            py36h8c6ec74_0  \r\n",
      "webencodings              0.5.1                    py36_1  \r\n",
      "Werkzeug                  0.14.1                    <pip>\r\n",
      "wheel                     0.32.3                   py36_0  \r\n",
      "widgetsnbextension        3.4.2                    py36_0  \r\n",
      "wrapt                     1.11.0           py36h1de35cc_0  \r\n",
      "xz                        5.2.4                h1de35cc_4  \r\n",
      "yaml                      0.1.7                hc338f04_2  \r\n",
      "zeromq                    4.2.5                h0a44026_1  \r\n",
      "zlib                      1.2.11               h1de35cc_3  \r\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_gcp_dl_2_0]",
   "language": "python",
   "name": "conda-env-env_gcp_dl_2_0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
