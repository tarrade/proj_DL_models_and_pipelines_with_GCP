{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification example with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages on Google  Cloud Datalab (locally use conda env)\n",
    "### Select in the Python3 Kernel:\n",
    "In the menu bar the of 'Kernel', select   \n",
    "**python3**\n",
    "### Install needed packages\n",
    "copy the command below in a Google Cloud Datalab cell  \n",
    "**!pip install tensorflow==1.12**\n",
    "### Restart the Kernel \n",
    "this is to take into account the new installed packages. Click in the menu bar on:  \n",
    "**Reset Session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needed librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gzip\n",
    "import sys\n",
    "import _pickle as cPickle\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import json \n",
    "import subprocess\n",
    "import requests\n",
    "import google.auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mnist data, split between train and test sets\n",
    "# on GCP\n",
    "#(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# with AXA network\n",
    "def load_data(path):\n",
    "    f = gzip.open(path, 'rb')\n",
    "    if sys.version_info < (3,):\n",
    "        data = cPickle.load(f)\n",
    "    else:\n",
    "        data = cPickle.load(f, encoding='bytes')\n",
    "    f.close()\n",
    "    return data\n",
    "(x_train, y_train), (x_test, y_test) = load_data(path='../data/mnist.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data shape (training)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data shape (train)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('uint8'), dtype('uint8'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype, x_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 0, 255, 0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(x_train), np.min(x_train), np.max(x_test), np.min(x_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.476806640625"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.nbytes/1024.0**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.86083984375"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.nbytes/1024.0**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0095367431640625"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.nbytes/1024.0**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057220458984375"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.nbytes/1024.0**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize and reorganize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast uint8 -> float32\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renormalize the data 255 grey variation\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data 28 x 28 -> 784\n",
    "x_train = x_train.reshape(len(x_train), x_train.shape[1]*x_train.shape[2])\n",
    "x_test = x_test.reshape(len(x_test), x_test.shape[1]*x_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train), np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_input=x_train.shape[1]\n",
    "dim_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, label) in enumerate(zip(x_train[0:5], y_train[0:5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Training: %i\\n' % np.argmax(label), fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "learning_rate = 0.5\n",
    "\n",
    "# number of epoch to train our model\n",
    "EPOCHS = 2\n",
    "\n",
    "# size of our mini batch\n",
    "#BATCH_SIZE = len(x_train)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# shuffle buffer size\n",
    "SHUFFLE_BUFFER_SIZE = 10 * BATCH_SIZE\n",
    "\n",
    "# prefetch buffer size\n",
    "PREFETCH_BUFFER_SIZE = 1 #tf.contrib.data.AUTOTUNE\n",
    "\n",
    "# number of paralell calls\n",
    "NUM_PARALELL_CALL = 4\n",
    "\n",
    "# hidden layer 1\n",
    "n1=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "    \n",
    "del_all_flags(tf.flags.FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel') # just for jupyter notebook and avoir : \"UnrecognizedFlagError: Unknown command line flag 'f'\"\n",
    "tf.app.flags.DEFINE_string('model_dir', '../results/Models/Mnist/tf_1_12/estimator/ckpt/', 'Dir to save a model and checkpoints')\n",
    "tf.app.flags.DEFINE_string('saved_dir', '../results/Models/Mnist/tf_1_12/estimator/pt/', 'Dir to save a model for TF serving')\n",
    "tf.app.flags.DEFINE_string('model_dir_keras', '../results/Models/Mnist/tf_1_12/keras/ckpt/', 'Dir to save a model and checkpoints with keras')\n",
    "tf.app.flags.DEFINE_string('tensorboard_dir_keras', '../results/Models/Mnist/tf_1_12/keras/logs/', 'Dir to save logs for TensorBoard with keras')\n",
    "tf.app.flags.DEFINE_integer('shuffle_buffer_size', SHUFFLE_BUFFER_SIZE , 'Shuffle buffer size')\n",
    "tf.app.flags.DEFINE_integer('prefetch_buffer_size', PREFETCH_BUFFER_SIZE, 'Prefetch buffer size')\n",
    "tf.app.flags.DEFINE_integer('batch_size', BATCH_SIZE, 'Batch size')\n",
    "tf.app.flags.DEFINE_integer('epoch', EPOCHS, 'number of epoch')\n",
    "tf.app.flags.DEFINE_integer('num_parallel_calls', NUM_PARALELL_CALL, 'Number of paralell calls')\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/tarrade/anaconda3/envs/env_gcp_dl/lib/python3.6/site-packages/ipykernel_launcher.py:\n",
      "  --batch_size: Batch size\n",
      "    (default: '128')\n",
      "    (an integer)\n",
      "  --epoch: number of epoch\n",
      "    (default: '2')\n",
      "    (an integer)\n",
      "  --f: kernel\n",
      "    (default: '')\n",
      "  --model_dir: Dir to save a model and checkpoints\n",
      "    (default: '../results/Models/Mnist/tf_1_12/estimator/ckpt/')\n",
      "  --model_dir_keras: Dir to save a model and checkpoints with keras\n",
      "    (default: '../results/Models/Mnist/tf_1_12/keras/ckpt/')\n",
      "  --num_parallel_calls: Number of paralell calls\n",
      "    (default: '4')\n",
      "    (an integer)\n",
      "  --prefetch_buffer_size: Prefetch buffer size\n",
      "    (default: '1')\n",
      "    (an integer)\n",
      "  --saved_dir: Dir to save a model for TF serving\n",
      "    (default: '../results/Models/Mnist/tf_1_12/estimator/pt/')\n",
      "  --shuffle_buffer_size: Shuffle buffer size\n",
      "    (default: '1280')\n",
      "    (an integer)\n",
      "  --tensorboard_dir_keras: Dir to save logs for TensorBoard with keras\n",
      "    (default: '../results/Models/Mnist/tf_1_12/keras/logs/')\n",
      "\n",
      "absl.flags:\n",
      "  --flagfile: Insert flag definitions from the given file into the command line.\n",
      "    (default: '')\n",
      "  --undefok: comma-separated list of flag names that it is okay to specify on\n",
      "    the command line even if the program does not define a flag with that name.\n",
      "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\n",
      "    format.\n",
      "    (default: '')\n"
     ]
    }
   ],
   "source": [
    "print(FLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data.Dataset\n",
    "https://www.tensorflow.org/guide/performance/datasets  \n",
    "To summarize, one good order for the different transformations is:\n",
    "- create the dataset\n",
    "- shuffle (with a big enough buffer size https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle)\n",
    "- repeat\n",
    "- map with the actual work (preprocessing, augmentationâ€¦) using multiple parallel calls\n",
    "- batch\n",
    "- prefetch\n",
    "\n",
    "ModeKeys:  \n",
    "https://www.tensorflow.org/api_docs/python/tf/estimator/ModeKeys  \n",
    "- EVAL\n",
    "- PREDICT\n",
    "- TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('eval', 'infer', 'train')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.estimator.ModeKeys.EVAL, tf.estimator.ModeKeys.PREDICT, tf.estimator.ModeKeys.TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_dataset_fn(x_data, y_data, batch_size=128, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        tf.logging.info(\"input_dataset_fn: PREDICT, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        tf.logging.info(\"input_dataset_fn: EVAL, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        tf.logging.info(\"input_dataset_fn: TRAIN, {}\".format(mode))\n",
    "    \n",
    "    # 1) convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "    \n",
    "    # 2) shuffle (with a big enough buffer size)    :        \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        #num_epochs = None # loop indefinitely\n",
    "        num_epochs = FLAGS.epoch\n",
    "        dataset = dataset.shuffle(buffer_size=FLAGS.shuffle_buffer_size, seed=2)# depends on sample size\n",
    "    else:\n",
    "        #num_epochs = 1 # end-of-input after this\n",
    "        num_epochs = FLAGS.epoch\n",
    "        \n",
    "    print('the number of epoch: num_epoch =', num_epochs)\n",
    "        \n",
    "    # caching data\n",
    "    #dataset = dataset.cache()\n",
    "    \n",
    "    # 3) automatically refill the data queue when empty\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    \n",
    "    # 4) map\n",
    "    #dataset = dataset.map(map_func=parse_fn, num_parallel_calls=FLAGS.num_parallel_calls)\n",
    "\n",
    "    # 5) create batches of data\n",
    "    dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "    \n",
    "    # 6) prefetch data for faster consumption, based on your system and environment, allows the tf.data runtime to automatically tune the prefetch buffer sizes\n",
    "    dataset = dataset.prefetch(FLAGS.prefetch_buffer_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: TRAIN, train\n",
      "the number of epoch: num_epoch = 2\n",
      "INFO:tensorflow:input_dataset_fn: EVAL, eval\n",
      "the number of epoch: num_epoch = 2\n"
     ]
    }
   ],
   "source": [
    "training_dataset = input_dataset_fn(x_train, \n",
    "                                    y_train, \n",
    "                                    mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                                    batch_size=FLAGS.batch_size)\n",
    "testing_dataset = input_dataset_fn(x_test, \n",
    "                                   y_test,\n",
    "                                   mode=tf.estimator.ModeKeys.EVAL, \n",
    "                                   batch_size=len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = training_dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_element\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration n: 0 execution time: 19.241626999999998 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 1 execution time: 0.004407000000000494 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 2 \n",
      "\n",
      "iteration n: 2 execution time: 0.0030760000000000787 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 6 \n",
      "\n",
      "iteration n: 3 execution time: 0.0047330000000016526 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 1 \n",
      "\n",
      "iteration n: 4 execution time: 0.004023999999994032 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 5 \n",
      "\n",
      "iteration n: 5 execution time: 0.004402999999996382 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 1 \n",
      "\n",
      "iteration n: 6 execution time: 0.004941000000002305 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 7 execution time: 0.004837000000001979 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 8 \n",
      "\n",
      "iteration n: 8 execution time: 0.005237000000001046 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 2 \n",
      "\n",
      "iteration n: 9 execution time: 0.006029000000005169 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 3 \n",
      "\n",
      "number of iteration reached\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "# maximum: EPOCHS*len(x_train)//BATCH_SIZE\n",
    "n_iter=10\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = sess.run([features, labels])\n",
    "            print('iteration n:', n, 'execution time:', time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "            n+=1\n",
    "            if n>=n_iter:\n",
    "                print('number of iteration reached')\n",
    "                break\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = testing_dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_element\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4615380000000044 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "0.1018100000000004 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "tf.errors.OutOfRangeError\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = sess.run([features, labels])\n",
    "            print(time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning modelling with Keras\n",
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../results/Models/Mnist/tf_1_12/keras/ckpt/'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAGS.model_dir_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(FLAGS.model_dir_keras, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def baseline_model(opt='tf'):\n",
    "    # create model\n",
    "    model = tf.keras.Sequential()\n",
    " \n",
    "    # hidden layer\n",
    "    model.add(tf.keras.layers.Dense(dim_input, \n",
    "                    input_dim=dim_input, \n",
    "                    kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                    bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                    activation='relu'))\n",
    "    # last layer\n",
    "    model.add(tf.keras.layers.Dense(num_classes, \n",
    "                    kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                    bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                    activation='softmax'))\n",
    "    \n",
    "    # weight initialisation\n",
    "    # He: keras.initializers.he_normal(seed=None)\n",
    "    # Xavier: keras.initializers.glorot_uniform(seed=None)\n",
    "    # Radom Normal: keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "    # Truncated Normal: keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "    \n",
    "    if opt=='keras':\n",
    "        optimiser=tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9)\n",
    "        # GD/SGC:   keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "        # Adam:     keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "        # RMSProp:  keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "        # Momentum: keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    else:\n",
    "        #optimiser (use tf.train and not tf.keras to use MirrorStrategy)\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/train/Optimizer\n",
    "        optimiser=tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9)\n",
    "        # GD/SGC:   tf.train.GradientDescentOptimizer(learning_rate, use_locking=False, name='GradientDescent') \n",
    "        # Adam:     tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False,name='Adam')\n",
    "        # RMSProp:  tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.0, epsilon=1e-10, use_locking=False, centered=False, name='RMSProp')\n",
    "        # Momentum: tf.train.MomentumOptimizer(learning_rate, momentum, use_locking=False, name='Momentum', use_nesterov=False)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimiser, \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the model\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "# keras optimiser\n",
    "model_opt_keras = baseline_model(opt='keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the nuber of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 623,290\n",
      "Trainable params: 623,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_opt_keras.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check input and output layer names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_input']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt_keras.input_names # Use this name as the dictionary key in the TF input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_1']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt_keras.output_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call back with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print info during iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UDFPrint(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        _, __, x_train, y_train = self.test_data\n",
    "        loss_train, acc_train = self.model.evaluate(x_train, y_train, verbose=0)\n",
    "        print('Reached epoch {0:3d} cost J = {1:.5f}'.format(epoch, loss_train))\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x_test, y_test, x_train, y_train = self.test_data\n",
    "        loss_train, acc_train = self.model.evaluate(x_train, y_train, verbose=0)\n",
    "        loss_test, acc_test = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        print(' accurary on the training set {0:.4f}'.format(acc_train))\n",
    "        print(' accurary on the testing set {0:.4f}'.format(acc_test))\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        #print('  ---> starting minibatch', batch)\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        #print('  ---> ending minibatch', batch)\n",
    "        return\n",
    "return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../results/Models/Mnist/tf_1_12/keras/logs/'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAGS.tensorboard_dir_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events.out.tfevents.1549038093.Fabien-Tarrades-MacBook-Pro.local\n",
      "events.out.tfevents.1549050335.Fabien-Tarrades-MacBook-Pro.local\n",
      "events.out.tfevents.1549038351.Fabien-Tarrades-MacBook-Pro.local\n",
      "events.out.tfevents.1549050551.Fabien-Tarrades-MacBook-Pro.local\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(FLAGS.tensorboard_dir_keras+'*'):\n",
    "    print(re.findall(r'[^\\\\/]+|[\\\\/]',file)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(FLAGS.tensorboard_dir_keras+'*' ,ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "tbCallBack=tf.keras.callbacks.TensorBoard(log_dir=FLAGS.tensorboard_dir_keras, \n",
    "                                          histogram_freq=1, \n",
    "                                          write_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = model_opt_keras.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and inference using Keras\n",
    "https://www.tensorflow.org/guide/keras\n",
    "- batch_size \n",
    "  determines the number of samples in each mini batch. Its maximum is the number of all samples, which makes gradient descent accurate, the loss will decrease towards the minimum if the learning rate is small enough, but iterations are slower. Its minimum is 1, resulting in stochastic gradient descent: Fast but the direction of the gradient step is based only on one example, the loss may jump around. batch_size allows to adjust between the two extremes: accurate gradient direction and fast iteration. Also, the maximum value for batch_size may be limited if your model + data set does not fit into the available (GPU) memory.\n",
    "- steps_per_epoch \n",
    "  the number of batch iterations before a training epoch is considered finished. If you have a training set of fixed size you can ignore it but it may be useful if you have a huge data set or if you are generating random data augmentations on the fly, i.e. if your training set has a (generated) infinite size. If you have the time to go through your whole training data set I recommend to skip this parameter.\n",
    "- validation_steps \n",
    "  similar to steps_per_epoch but on the validation data set instead on the training data. If you have the time to go through your whole validation data set I recommend to skip this parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Reached epoch   0 cost J = 2.35535\n",
      "Epoch 1/2\n",
      " accurary on the training set 0.9626\n",
      " accurary on the testing set 0.9548\n",
      " - 104s - loss: 0.2118 - acc: 0.9360 - val_loss: 0.1453 - val_acc: 0.9548\n",
      "Reached epoch   1 cost J = 0.11518\n",
      "Epoch 2/2\n",
      " accurary on the training set 0.9798\n",
      " accurary on the testing set 0.9725\n",
      " - 103s - loss: 0.1113 - acc: 0.9668 - val_loss: 0.1078 - val_acc: 0.9725\n"
     ]
    }
   ],
   "source": [
    "# Set to the original weights for testing other pipelines\n",
    "model_opt_keras.set_weights(initial_weights)\n",
    "\n",
    "# Fit the model\n",
    "hist=model_opt_keras.fit(x_train, # use batch_size\n",
    "                         y_train, # use batch_size\n",
    "                         validation_data=(x_test, y_test), # use full validation step at the end of each epoch\n",
    "                         callbacks=[UDFPrint((x_test, y_test, x_train, y_train)), tbCallBack],\n",
    "                         epochs=EPOCHS, \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.14530197284817695, 0.107827948564291], 'val_acc': [0.9548, 0.9725], 'loss': [0.21180632532636326, 0.11132152475019295], 'acc': [0.9359500000317892, 0.9667666666348775]}\n"
     ]
    }
   ],
   "source": [
    "print(hist.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.10782794496663846\n",
      "Test accuracy: 0.9725\n"
     ]
    }
   ],
   "source": [
    "score = model_opt_keras.evaluate(x_test, \n",
    "                                 y_test, \n",
    "                                 verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.06794429655395409\n",
      "Train accuracy: 0.97975\n"
     ]
    }
   ],
   "source": [
    "score = model_opt_keras.evaluate(x_train, \n",
    "                                 y_train, \n",
    "                                 verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with keras optimiser we can save the model+weight\n",
    "if not os.path.exists(FLAGS.model_dir_keras):\n",
    "    os.makedirs(FLAGS.model_dir_keras)\n",
    "model_opt_keras.save(FLAGS.model_dir_keras+'keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload the model and make evaluation using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_model_opt_keras=tf.keras.models.load_model(FLAGS.model_dir_keras+'keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.06794429655395409\n",
      "Train accuracy: 0.97975\n"
     ]
    }
   ],
   "source": [
    "score = reload_model_opt_keras.evaluate(x_train, \n",
    "                                        y_train, \n",
    "                                        verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring using TensorBoard"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start in a separte shell with the env activated:\n",
    "cd your the working dir pof the project\n",
    "tensorboard --logdir  \"./results/Models/Mnist/tf_1_12/keras/logs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and inference using  Keras and tf.data.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **batch_size** determines the number of samples in each mini batch. Its maximum is the number of all samples, which makes gradient descent accurate, the loss will decrease towards the minimum if the learning rate is small enough, but iterations are slower. Its minimum is 1, resulting in stochastic gradient descent: Fast but the direction of the gradient step is based only on one example, the loss may jump around. batch_size allows to adjust between the two extremes: accurate gradient direction and fast iteration. Also, the maximum value for batch_size may be limited if your model + data set does not fit into the available (GPU) memory.\n",
    "- **steps_per_epoch** the number of batch iterations before a training epoch is considered finished. If you have a training set of fixed size you can ignore it but it may be useful if you have a huge data set or if you are generating random data augmentations on the fly, i.e. if your training set has a (generated) infinite size. If you have the time to go through your whole training data set I recommend to skip this parameter.\n",
    "- **validation_steps** similar to steps_per_epoch but on the validation data set instead on the training data. If you have the time to go through your whole validation data set I recommend to skip this parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of events: 60000\n",
      "number of epoch: 2\n",
      "number of batch size: 128\n",
      "number of batches per epoch: 468\n",
      "number of BATCH_SIZE*len(x_train) // BATCH_SIZE* 59904\n"
     ]
    }
   ],
   "source": [
    "print('number of events:', len(x_train))\n",
    "print('number of epoch:', EPOCHS)\n",
    "print('number of batch size:', BATCH_SIZE)\n",
    "print('number of batches per epoch:', len(x_train) // BATCH_SIZE)\n",
    "print('number of BATCH_SIZE*len(x_train) // BATCH_SIZE*', BATCH_SIZE*(len(x_train) // BATCH_SIZE) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of events: 10000\n",
      "number of epoch: 2\n",
      "number of batch size: 128\n",
      "number of batches per epoch: 78\n",
      "number of BATCH_SIZE*len(x_train) // BATCH_SIZE* 9984\n"
     ]
    }
   ],
   "source": [
    "print('number of events:', len(x_test))\n",
    "print('number of epoch:', EPOCHS)\n",
    "print('number of batch size:', BATCH_SIZE)\n",
    "print('number of batches per epoch:', len(x_test) // BATCH_SIZE)\n",
    "print('number of BATCH_SIZE*len(x_train) // BATCH_SIZE*', BATCH_SIZE*(len(x_test) // BATCH_SIZE) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model using Keras and tf.data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached epoch   0 cost J = 2.35535\n",
      "Epoch 1/2\n",
      " accurary on the training set 0.9629\n",
      " accurary on the testing set 0.9561\n",
      " - 130s - loss: 0.2367 - acc: 0.9337 - val_loss: 0.1508 - val_acc: 0.9561\n",
      "Reached epoch   1 cost J = 0.12567\n",
      "Epoch 2/2\n",
      " accurary on the training set 0.9703\n",
      " accurary on the testing set 0.9623\n",
      " - 111s - loss: 0.1057 - acc: 0.9679 - val_loss: 0.1446 - val_acc: 0.9623\n"
     ]
    }
   ],
   "source": [
    "# Set to the original weights for testing other pipelines\n",
    "model_opt_keras.set_weights(initial_weights)\n",
    "\n",
    "# Fit the model (using data.Dataset)\n",
    "hist=model_opt_keras.fit(training_dataset.make_one_shot_iterator(), # use batch_size\n",
    "                         steps_per_epoch=len(x_train) // BATCH_SIZE,\n",
    "                         validation_data=testing_dataset.make_one_shot_iterator(), # use full validation dataset\n",
    "                         validation_steps=1, \n",
    "                         callbacks=[UDFPrint((x_test, y_test, x_train, y_train)), tbCallBack],\n",
    "                         epochs=EPOCHS,\n",
    "                         verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.15084655582904816, 0.14456292986869812], 'val_acc': [0.9560999870300293, 0.9623000025749207], 'loss': [0.23667740234388754, 0.10565868438555835], 'acc': [0.9336605235042735, 0.9678652510683761]}\n"
     ]
    }
   ],
   "source": [
    "print(hist.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model using Keras and tf.data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.14456331603423458\n",
      "Test accuracy: 0.9623\n"
     ]
    }
   ],
   "source": [
    "score = model_opt_keras.evaluate(x_test, \n",
    "                       y_test, \n",
    "                       verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09529252001663747\n",
      "Train accuracy: 0.9703\n"
     ]
    }
   ],
   "source": [
    "score = model_opt_keras.evaluate(x_train, \n",
    "                       y_train, \n",
    "                       verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and inference using tf.estimator and tf.data.dataset\n",
    "### Convert Keras model to work with tf.train.optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf optimiser\n",
    "model_opt_tf = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 623,290\n",
      "Trainable params: 623,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_opt_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_2_input']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt_tf.input_names # Use this name as the dictionary key in the TF input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_3']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt_tf.output_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeHistory(tf.train.SessionRunHook):\n",
    "    def begin(self):\n",
    "        self.times = []\n",
    "\n",
    "    def before_run(self, run_context):\n",
    "        self.iter_time_start = time.time()\n",
    "\n",
    "    def after_run(self, run_context, run_values):\n",
    "        self.times.append(time.time() - self.iter_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_hist = TimeHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf.distribute.startegy work across multiple devices/machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    }
   ],
   "source": [
    "# the tf.distribute.Strategy API is an easy way to distribute your training across multiple devices/machines\n",
    "\n",
    "#strategy=None\n",
    "## work with Keras with tf.train optimiser not tf.keras\n",
    "strategy = tf.contrib.distribute.OneDeviceStrategy('device:CPU:0')\n",
    "#strategy = tf.contrib.distribute.OneDeviceStrategy('device:GPU:0')\n",
    "#NUM_GPUS = 2\n",
    "#strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS)\n",
    "#strategy = tf.contrib.distribute.MirroredStrategy()\n",
    "\n",
    "# config tf.estimator to use a give strategy\n",
    "training_config = tf.estimator.RunConfig(train_distribute=strategy,\n",
    "                                         model_dir=FLAGS.model_dir,\n",
    "                                         save_summary_steps=20,\n",
    "                                         save_checkpoints_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform a keras model to estimator model\n",
    "delete fist the folder for a clean start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../results/Models/Mnist/tf_1_12/estimator/ckpt/'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAGS.model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras\n",
      "model.ckpt-880.meta\n",
      "model.ckpt-860.data-00000-of-00001\n",
      "checkpoint\n",
      "model.ckpt-880.data-00000-of-00001\n",
      "model.ckpt-920.meta\n",
      "model.ckpt-900.meta\n",
      "model.ckpt-937.index\n",
      "model.ckpt-900.data-00000-of-00001\n",
      "model.ckpt-937.meta\n",
      "graph.pbtxt\n",
      "model.ckpt-900.index\n",
      "events.out.tfevents.1549050797.Fabien-Tarrades-MacBook-Pro.local\n",
      "model.ckpt-937.data-00000-of-00001\n",
      "model.ckpt-920.index\n",
      "model.ckpt-920.data-00000-of-00001\n",
      "eval\n",
      "model.ckpt-880.index\n",
      "model.ckpt-860.index\n",
      "model.ckpt-860.meta\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(FLAGS.model_dir+'*'):\n",
    "    print(re.findall(r'[^\\\\/]+|[\\\\/]',file)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(FLAGS.model_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../results/Models/Mnist/tf_1_12/estimator/ckpt/', '_tf_random_seed': None, '_save_summary_steps': 20, '_save_checkpoints_steps': 20, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.one_device_strategy.OneDeviceStrategy object at 0xb47d2bef0>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb47d2bf60>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n"
     ]
    }
   ],
   "source": [
    "# Set to the original weights for testing other pipelines\n",
    "model_opt_tf.set_weights(initial_weights)\n",
    "\n",
    "# transfor keras model to estimator model\n",
    "estimator_train_model = tf.keras.estimator.model_to_estimator(keras_model=model_opt_tf,\n",
    "                                                              config=training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model using Keras, tf.estimator and tf.data.dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train(\n",
    "    input_fn,\n",
    "    hooks=None,\n",
    "    steps=None,\n",
    "    max_steps=None,\n",
    "    saving_listeners=None\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# option 1:\n",
    "lambda:input_dataset_fn(x_train,\n",
    "                            y_train, \n",
    "                            mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                            batch_size=FLAGS.batch_size)\n",
    "# option 2\n",
    "def get_train_input_fn():\n",
    "    return input_dataset_fn(x_train,\n",
    "                            y_train, \n",
    "                            mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                            batch_size=FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: TRAIN, train\n",
      "the number of epoch: num_epoch = 2\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='../results/Models/Mnist/tf_1_12/estimator/ckpt/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: ('../results/Models/Mnist/tf_1_12/estimator/ckpt/keras/keras_model.ckpt',)\n",
      "INFO:tensorflow:Warm-starting variable: dense_2/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_2/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_3/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_3/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3301468, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 20 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 40 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 60 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 80 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 100 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 5.46311\n",
      "INFO:tensorflow:loss = 0.30597115, step = 100 (18.306 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 120 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 140 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 160 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 180 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 200 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 4.79778\n",
      "INFO:tensorflow:loss = 0.20820636, step = 200 (20.842 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 220 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 240 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 260 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 280 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 300 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 4.08826\n",
      "INFO:tensorflow:loss = 0.14536124, step = 300 (24.462 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 320 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 340 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 360 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 380 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 400 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.95786\n",
      "INFO:tensorflow:loss = 0.118823946, step = 400 (25.265 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 420 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 440 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 460 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 480 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 500 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 4.21719\n",
      "INFO:tensorflow:loss = 0.060583837, step = 500 (23.711 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 520 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 540 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 560 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 580 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 600 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 6.16367\n",
      "INFO:tensorflow:loss = 0.07921085, step = 600 (16.224 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 620 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 640 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 660 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 680 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 700 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 6.13631\n",
      "INFO:tensorflow:loss = 0.06226484, step = 700 (16.296 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 720 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 740 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 760 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 780 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 800 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 5.20393\n",
      "INFO:tensorflow:loss = 0.024773862, step = 800 (19.216 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 820 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 840 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 860 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 880 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 900 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 5.09446\n",
      "INFO:tensorflow:loss = 0.13263443, step = 900 (19.629 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 920 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 937 into ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Finalize system.\n",
      "INFO:tensorflow:Loss for final step: 0.11366376.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0xb6beecf98>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model (using estimator.train and data.Dataset)\n",
    "estimator_train_model.train(input_fn=lambda:input_dataset_fn(x_train, y_train,mode=tf.estimator.ModeKeys.TRAIN, batch_size=FLAGS.batch_size),\n",
    "                            steps=1000,\n",
    "                            hooks=[time_hist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\n",
      "keras_model.ckpt.meta\n",
      "keras_model.ckpt.index\n",
      "keras_model.ckpt.data-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(FLAGS.model_dir+'keras/*'):\n",
    "    print(re.findall(r'[^\\\\/]+|[\\\\/]',file)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras\n",
      "model.ckpt-880.meta\n",
      "model.ckpt-860.data-00000-of-00001\n",
      "checkpoint\n",
      "model.ckpt-880.data-00000-of-00001\n",
      "model.ckpt-920.meta\n",
      "model.ckpt-900.meta\n",
      "model.ckpt-937.index\n",
      "model.ckpt-900.data-00000-of-00001\n",
      "model.ckpt-937.meta\n",
      "graph.pbtxt\n",
      "model.ckpt-900.index\n",
      "events.out.tfevents.1549054529.Fabien-Tarrades-MacBook-Pro.local\n",
      "model.ckpt-937.data-00000-of-00001\n",
      "model.ckpt-920.index\n",
      "model.ckpt-920.data-00000-of-00001\n",
      "model.ckpt-880.index\n",
      "model.ckpt-860.index\n",
      "model.ckpt-860.meta\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(FLAGS.model_dir+'*'):\n",
    "    print(re.findall(r'[^\\\\/]+|[\\\\/]',file)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time with the current strategy: 188.78705406188965 seconds\n"
     ]
    }
   ],
   "source": [
    "total_time = sum(time_hist.times)\n",
    "print(f\"total time with the current strategy: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635.2977993961474 images/second\n"
     ]
    }
   ],
   "source": [
    "avg_time_per_batch = np.mean(time_hist.times)\n",
    "print(f\"{BATCH_SIZE/avg_time_per_batch} images/second\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start in a separte shell with the env activated:\n",
    "tensorboard --logdir\"./results/Models/Mnist/tf_1_12/estimator/ckpt/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model using Keras, tf.estimator and tf.data.dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "evaluate(\n",
    "    input_fn,\n",
    "    steps=None,\n",
    "    hooks=None,\n",
    "    checkpoint_path=None,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: TRAIN, train\n",
      "the number of epoch: num_epoch = 2\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-01-21:00:28\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt-937\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-01-21:00:43\n",
      "INFO:tensorflow:Saving dict for global step 937: accuracy = 0.97863334, global_step = 937, loss = 0.07083233\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 937: ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt-937\n"
     ]
    }
   ],
   "source": [
    "score=estimator_train_model.evaluate(input_fn=lambda:input_dataset_fn(x_train,y_train, mode=tf.estimator.ModeKeys.TRAIN, batch_size=len(y_train)),\n",
    "                                     steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.97863334, 'loss': 0.07083233, 'global_step': 937}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07083233\n",
      "Train accuracy: 0.97863334\n",
      "Train global steps: 937\n"
     ]
    }
   ],
   "source": [
    "print('Train loss:', score['loss'])\n",
    "print('Train accuracy:', score['accuracy'])\n",
    "print('Train global steps:', score['global_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: EVAL, eval\n",
      "the number of epoch: num_epoch = 2\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-01-21:00:48\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt-937\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-01-21:00:50\n",
      "INFO:tensorflow:Saving dict for global step 937: accuracy = 0.9704, global_step = 937, loss = 0.098214075\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 937: ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt-937\n"
     ]
    }
   ],
   "source": [
    "score=estimator_train_model.evaluate(input_fn=lambda:input_dataset_fn(x_test, y_test,mode=tf.estimator.ModeKeys.EVAL, batch_size=len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.098214075\n",
      "Test accuracy: 0.9704\n",
      "Test global steps: 937\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', score['loss'])\n",
    "print('Test accuracy:', score['accuracy'])\n",
    "print('Test global steps:', score['global_step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using the Keras model, tf.estimator and tf.data.dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predict(\n",
    "    input_fn,\n",
    "    predict_keys=None,\n",
    "    hooks=None,\n",
    "    checkpoint_path=None,\n",
    "    yield_single_examples=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: EVAL, eval\n",
      "the number of epoch: num_epoch = 2\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt-937\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predictions=list(estimator_train_model.predict(input_fn=lambda:input_dataset_fn(x_test, y_test,mode=tf.estimator.ModeKeys.EVAL, batch_size=len(x_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dense_3'])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer=model_opt_tf.output_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 7\n",
      "Predicted label:  7\n",
      "Actual label: 2\n",
      "Predicted label:  2\n",
      "Actual label: 1\n",
      "Predicted label:  1\n",
      "Actual label: 0\n",
      "Predicted label:  0\n",
      "Actual label: 4\n",
      "Predicted label:  4\n",
      "Actual label: 1\n",
      "Predicted label:  1\n",
      "Actual label: 4\n",
      "Predicted label:  4\n",
      "Actual label: 9\n",
      "Predicted label:  9\n",
      "Actual label: 5\n",
      "Predicted label:  5\n",
      "Actual label: 9\n",
      "Predicted label:  9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    prediction_array = predictions[i][output_layer]\n",
    "    predicted_label = np.argmax(prediction_array)\n",
    "    print('Actual label:', np.argmax(y_test[i]))\n",
    "    print(\"Predicted label: \", predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras's model checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 623,290\n",
      "Trainable params: 623,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_opt_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_2_input']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt_tf.input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_3']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt_tf.output_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator's model checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_2/bias',\n",
       " 'dense_2/bias/Adam',\n",
       " 'dense_2/bias/Adam_1',\n",
       " 'dense_2/kernel',\n",
       " 'dense_2/kernel/Adam',\n",
       " 'dense_2/kernel/Adam_1',\n",
       " 'dense_3/bias',\n",
       " 'dense_3/bias/Adam',\n",
       " 'dense_3/bias/Adam_1',\n",
       " 'dense_3/kernel',\n",
       " 'dense_3/kernel/Adam',\n",
       " 'dense_3/kernel/Adam_1',\n",
       " 'global_step',\n",
       " 'training/TFOptimizer/beta1_power',\n",
       " 'training/TFOptimizer/beta2_power']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_train_model.get_variable_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_train_model.get_variable_value(estimator_train_model.get_variable_names()[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_train_model.get_variable_value(estimator_train_model.get_variable_names()[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt-937'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_train_model.latest_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model serving using Keras, tf.estimator and tf.data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../results/Models/Mnist/tf_1_12/estimator/pt/'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAGS.saved_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(FLAGS.saved_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_2_input']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt_tf.input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    \"\"\"Serving input_fn that builds features from placeholders#\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.estimator.export.ServingInputReceiver\n",
    "    \"\"\"\n",
    "    input_images = tf.placeholder(tf.float32, [None, 784])\n",
    "    features = {'dense_2_input' : input_images} # this is the dict that is then passed as \"features\" parameter to your model_fn\n",
    "    receiver_tensors = {'dense_2_input': input_images} # As far as I understand this is needed to map the input to a name you can retrieve later\n",
    "   \n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(FLAGS.saved_dir):\n",
    "    os.makedirs(FLAGS.saved_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from ../results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt-937\n",
      "WARNING:tensorflow:From /Users/tarrade/anaconda3/envs/env_gcp_dl/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py:1044: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/results/Models/Mnist/tf_1_12/estimator/pt/temp-b'1549054852'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'/Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/results/Models/Mnist/tf_1_12/estimator/pt/1549054852'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 2: Only export predict mode\n",
    "estimator_train_model.export_saved_model(os.path.abspath(FLAGS.saved_dir), \n",
    "                                         serving_input_receiver_fn=serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1549054852\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(FLAGS.saved_dir+'*'):\n",
    "    print(re.findall(r'[^\\\\/]+|[\\\\/]',file)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update the model id in the path below with the correct one from above i.e'1549040172'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tag='1549054852'\n",
    "os.environ['MODEL_FOR_SERVING']=FLAGS.saved_dir+model_tag+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables\n",
      "saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "# replace the folder name below with the one from above i.e '1549040172'\n",
    "for file in glob.glob(FLAGS.saved_dir+model_tag+'/*'):\n",
    "    print(re.findall(r'[^\\\\/]+|[\\\\/]',file)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the saved model before serving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('serving_default',)"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\r\n",
      "  inputs['dense_2_input'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (-1, 784)\r\n",
      "      name: Placeholder:0\r\n",
      "The given SavedModel SignatureDef contains the following output(s):\r\n",
      "  outputs['dense_3'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (-1, 10)\r\n",
      "      name: dense_3/Softmax:0\r\n",
      "Method name is: tensorflow/serving/predict\r\n"
     ]
    }
   ],
   "source": [
    "! saved_model_cli show --dir $MODEL_FOR_SERVING --tag serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cloud ML Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking gcloud installation (SDK)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "install SDK"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in case of issue with \"Bad magic number in .pyc file\" -> mix python 2 and python 3\n",
    "which python -> path (should use python 3)\n",
    "export CLOUDSDK_PYTHON=path (should use python 3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gcloud init"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gcloud components list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gcloud components update"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gcloud auth configure-docker"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gcloud version"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gcloud info"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gcloud ml-engine versions create --help"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gcloud ml-engine local predict --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CLOUDSDK_PYTHON']='/Users/tarrade/anaconda3/bin/python'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a input json file and string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input needed to get prediction using ml-engine an option --json-instances: 'dense_2_input' for each new entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prediction=x_test[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = input_prediction.tolist()\n",
    "json_file = \"../data/input_predict_gcloud.json\" \n",
    "\n",
    "with codecs.open(json_file, 'w', encoding='utf-8') as f:\n",
    "    for el in data:\n",
    "        instance = {'dense_2_input': el}\n",
    "        json.dump(instance, f , sort_keys=True)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input needed to get prediction using ml-engine and cURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = input_prediction.tolist()\n",
    "json_file = \"../data/input_predict_cURL.json\" \n",
    "\n",
    "with codecs.open(json_file, 'w', encoding='utf-8') as f:\n",
    "    tmp={}\n",
    "    list_tmp=[]\n",
    "    for el in data:\n",
    "        tmp['dense_2_input']=el\n",
    "        list_tmp.append(tmp)\n",
    "    instance = {\"instances\": list_tmp}    \n",
    "    json.dump(instance, f , sort_keys=True)\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input needed to get prediction using ml-engine and requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_predict_request = json.dumps({\"signature_name\": \"serving_default\", \"instances\": input_prediction.tolist()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(img.reshape(28,28))\n",
    "    plt.axis('off')\n",
    "    plt.title('\\n\\n{}'.format(title), fontdict={'size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    show(x_test[i],'Test dataset, true label: '+str(np.argmax(y_test[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model inference using gcloud locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CLOUDSDK_PYTHON']='/Users/tarrade/anaconda3/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m 2019-02-07 14:47:57.372308: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n",
      "2019-02-07 14:47:57.372739: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\r\n",
      "\r\n",
      "DENSE_3\r\n",
      "[5.820228579977993e-06, 2.2560351453648764e-07, 3.986712181358598e-05, 0.00012092345423297957, 1.6469396513230095e-08, 4.26365613748203e-06, 4.727720437358585e-10, 0.999786913394928, 7.320999429794028e-06, 3.4697757655521855e-05]\r\n",
      "[1.374597104586428e-05, 0.004179574083536863, 0.9947308301925659, 0.0008424344123341143, 4.2751646667227305e-10, 0.00018749698938336223, 1.1633811482170131e-05, 2.49833398413557e-09, 3.442165325395763e-05, 7.398723300688914e-10]\r\n",
      "[1.2485852494137362e-05, 0.9962553977966309, 0.000608675240073353, 0.0001076246626325883, 0.0008343913941644132, 0.00012625956151168793, 2.7262765797786415e-05, 0.0016017641173675656, 0.00036735867615789175, 5.882246477995068e-05]\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine local predict --model-dir $MODEL_FOR_SERVING --json-instances ../data/input_predict_gcloud.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model using Cloud ML Engine\n",
    "- https://cloud.google.com/ml-engine/docs/v1/predict-request\n",
    "- https://cloud.google.com/ml-engine/docs/tensorflow/online-predict#requesting_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROJECT']=subprocess.run('gcloud config list project --format \"value(core.project)\"', shell=True, check=True, stdout=subprocess.PIPE).stdout.decode().replace('\\n', '')\n",
    "os.environ['MODEL']='mnist'\n",
    "os.environ['BUCKET']='gs://'+os.environ['YOUR_PROJECT']\n",
    "os.environ['VERSION']='v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsutil need python 2.7\n",
    "os.environ['CLOUDSDK_PYTHON']='/Users/tarrade/anaconda/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://../results/Models/Mnist/tf_1_12/estimator/pt/1549054852/saved_model.pb [Content-Type=application/octet-stream]...\n",
      "Copying file://../results/Models/Mnist/tf_1_12/estimator/pt/1549054852/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
      "Copying file://../results/Models/Mnist/tf_1_12/estimator/pt/1549054852/variables/variables.index [Content-Type=application/octet-stream]...\n",
      "\\ [3 files][  2.4 MiB/  2.4 MiB]                                                \n",
      "Operation completed over 3 objects/2.4 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r $MODEL_FOR_SERVING $BUCKET/model_dir_tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME   DEFAULT_VERSION_NAME\r\n",
      "mnist  v1\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine models list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when creating the model for the first time"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gcloud ml-engine models create ${MODEL} \\\n",
    "--regions us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a version and store the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ml-engine.versions.create) ALREADY_EXISTS: Field: version.name Error: A version with the same name already exists.\r\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\r\n",
      "  fieldViolations:\r\n",
      "  - description: A version with the same name already exists.\r\n",
      "    field: version.name\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine versions create ${VERSION} \\\n",
    "--model ${MODEL} \\\n",
    "--origin=${BUCKET}/model_dir_tmp/1549054852 \\\n",
    "--runtime-version=1.12 \\\n",
    "--staging-bucket=${BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the list of model in ML-Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME   DEFAULT_VERSION_NAME\n",
      "mnist  v1\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME  DEPLOYMENT_URI                                              STATE\r\n",
      "v1    gs://ml-productive-pipeline-53122/model_dir_tmp/1549054852  READY\r\n",
      "v2    gs://ml-productive-pipeline-53122/model_dir_tmp/1549054852  READY\r\n"
     ]
    }
   ],
   "source": [
    " !gcloud ml-engine versions list --model mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing our model using ML-Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENSE_3\r\n",
      "[5.820239039167063e-06, 2.2560371348845365e-07, 3.9867194573162124e-05, 0.00012092355609638616, 1.6469426711296364e-08, 4.263659775460837e-06, 4.727719882247072e-10, 0.9997867941856384, 7.321012162719853e-06, 3.4697819501161575e-05]\r\n",
      "[1.3745981959800702e-05, 0.004179579671472311, 0.9947307109832764, 0.0008424347033724189, 4.275147735821605e-10, 0.00018749696027953178, 1.163381057267543e-05, 2.4983288771096568e-09, 3.442161687416956e-05, 7.39872219046589e-10]\r\n",
      "[1.2485862498579081e-05, 0.9962552785873413, 0.0006086757639423013, 0.00010762474994407967, 0.0008343925583176315, 0.00012625991075765342, 2.7262842195341364e-05, 0.0016017662128433585, 0.0003673589671961963, 5.8822570281336084e-05]\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine predict --model=${MODEL} --version=${VERSION} --json-instances ../data/input_predict_gcloud.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing using RESTful API \n",
    "- https://www.tensorflow.org/serving/api_rest  \n",
    "RESTful API is an application program interface (API) that uses HTTP requests to GET, PUT, POST and DELETE data (Json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"name\": \"projects/ml-productive-pipeline-53122/models/mnist/versions/v2\",\r\n",
      "  \"deploymentUri\": \"gs://ml-productive-pipeline-53122/model_dir_tmp/1549054852\",\r\n",
      "  \"createTime\": \"2019-02-07T11:00:51Z\",\r\n",
      "  \"lastUseTime\": \"2019-02-07T13:48:20Z\",\r\n",
      "  \"runtimeVersion\": \"1.12\",\r\n",
      "  \"state\": \"READY\",\r\n",
      "  \"etag\": \"KoHdxAqPbTQ=\",\r\n",
      "  \"framework\": \"TENSORFLOW\",\r\n",
      "  \"machineType\": \"mls1-c1-m2\",\r\n",
      "  \"pythonVersion\": \"2.7\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!curl https://ml.googleapis.com/v1/projects/${PROJECT}/models/${MODEL}/versions/${VERSION} \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": [{\"dense_3\": [1.2485862498579081e-05, 0.9962552785873413, 0.0006086757639423013, 0.00010762474994407967, 0.0008343925583176315, 0.00012625991075765342, 2.7262842195341364e-05, 0.0016017662128433585, 0.00036735893809236586, 5.8822570281336084e-05]}, {\"dense_3\": [1.2485862498579081e-05, 0.9962552785873413, 0.0006086757639423013, 0.00010762474994407967, 0.0008343925583176315, 0.00012625991075765342, 2.7262842195341364e-05, 0.0016017662128433585, 0.00036735893809236586, 5.8822570281336084e-05]}, {\"dense_3\": [1.2485862498579081e-05, 0.9962552785873413, 0.0006086757639423013, 0.00010762474994407967, 0.0008343925583176315, 0.00012625991075765342, 2.7262842195341364e-05, 0.0016017662128433585, 0.0003673589671961963, 5.8822570281336084e-05]}]}"
     ]
    }
   ],
   "source": [
    "!curl -X POST \\\n",
    "https://ml.googleapis.com/v1/projects/${PROJECT}/models/${MODEL}/versions/${VERSION}:predict \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-d @../data/input_predict_cURL.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python and requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'dense_3': [5.820239039167063e-06,\n",
       "    2.2560371348845365e-07,\n",
       "    3.9867194573162124e-05,\n",
       "    0.00012092355609638616,\n",
       "    1.6469426711296364e-08,\n",
       "    4.263659775460837e-06,\n",
       "    4.727719882247072e-10,\n",
       "    0.9997867941856384,\n",
       "    7.321012162719853e-06,\n",
       "    3.4697819501161575e-05]},\n",
       "  {'dense_3': [1.3745981959800702e-05,\n",
       "    0.004179579671472311,\n",
       "    0.9947307109832764,\n",
       "    0.0008424347033724189,\n",
       "    4.275147735821605e-10,\n",
       "    0.00018749696027953178,\n",
       "    1.163381057267543e-05,\n",
       "    2.4983288771096568e-09,\n",
       "    3.442161687416956e-05,\n",
       "    7.39872219046589e-10]},\n",
       "  {'dense_3': [1.2485862498579081e-05,\n",
       "    0.9962552785873413,\n",
       "    0.0006086757639423013,\n",
       "    0.00010762474994407967,\n",
       "    0.0008343925583176315,\n",
       "    0.00012625991075765342,\n",
       "    2.7262842195341364e-05,\n",
       "    0.0016017662128433585,\n",
       "    0.0003673589671961963,\n",
       "    5.8822570281336084e-05]}]}"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://ml.googleapis.com/v1/projects/'+os.environ['PROJECT']+'/models/'+os.environ['MODEL']+'/versions/'+os.environ['VERSION']+':predict'\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization':  'Bearer {}'.format(subprocess.run('gcloud auth print-access-token', shell=True, check=True, stdout=subprocess.PIPE).stdout.decode().replace('\\n', ''))}\n",
    "\n",
    "json_response = requests.post(url=url, data=input_predict_request, headers=headers)\n",
    "json.loads(json_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = json.loads(json_response.text)['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0,3):\n",
    "    show(x_test[i], 'The model thought this was a {} , and it was actually a {}'.format(np.argmax(predictions[i]['dense_3']),np.argmax(y_test[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing using gRPC API \n",
    "- https://cloud.google.com/endpoints/docs/grpc/about-grpc  \n",
    "gRPC is a high performance, open-source universal RPC framework, developed by Google. In gRPC, a client application can directly call methods on a server application on a different machine as if it was a local object, making it easier to create distributed applications and services (Protobuf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'unknown' 1.12.0\r\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /Users/tarrade/anaconda3/envs/env_gcp_dl:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_tflow_select             2.3.0                       mkl  \n",
      "absl-py                   0.7.0                    py36_0  \n",
      "altair                    2.3.0                    pypi_0    pypi\n",
      "appnope                   0.1.0            py36hf537a9a_0  \n",
      "asn1crypto                0.24.0                   py36_0  \n",
      "astor                     0.7.1                    py36_0  \n",
      "astroid                   2.1.0                    py36_0  \n",
      "atomicwrites              1.2.1                    py36_0  \n",
      "attrs                     18.2.0           py36h28b3542_0  \n",
      "autopep8                  1.4.3                    py36_0  \n",
      "backcall                  0.1.0                    py36_0  \n",
      "blas                      1.1                    openblas    conda-forge\n",
      "bleach                    3.1.0                    pypi_0    pypi\n",
      "c-ares                    1.15.0               h1de35cc_1  \n",
      "ca-certificates           2018.12.5                     0  \n",
      "certifi                   2018.11.29               py36_0  \n",
      "cffi                      1.11.5           py36h6174b99_1  \n",
      "chardet                   3.0.4                    py36_1  \n",
      "colorama                  0.4.1                    py36_0  \n",
      "cryptography              2.3.1            py36hdbc3d79_0  \n",
      "cycler                    0.10.0           py36hfc81398_0  \n",
      "dbus                      1.13.6               h90a0687_0  \n",
      "decorator                 4.3.0                    py36_0  \n",
      "defusedxml                0.5.0                    py36_1  \n",
      "entrypoints               0.3                      py36_0  \n",
      "expat                     2.2.6                h0a44026_0  \n",
      "freetype                  2.9.1                hb4e5f40_0  \n",
      "gast                      0.2.2                    py36_0  \n",
      "gettext                   0.19.8.1             h15daf44_3  \n",
      "gitdb2                    2.0.5                    py36_0  \n",
      "gitpython                 2.1.11                   py36_0  \n",
      "glib                      2.56.2               hd9629dc_0  \n",
      "grpcio                    1.14.1           py36h9011c5e_0  \n",
      "h5py                      2.9.0            py36h3134771_0  \n",
      "hdf5                      1.10.4               hfa1e0ec_0  \n",
      "icu                       58.2                 h4b95b61_1  \n",
      "idna                      2.8                      py36_0  \n",
      "ipykernel                 4.10.0                   py36_0  \n",
      "ipython                   7.2.0            py36h39e3cac_0  \n",
      "ipython_genutils          0.2.0            py36h241746c_0  \n",
      "ipywidgets                7.4.1                    py36_0  \n",
      "isort                     4.3.4                    py36_0  \n",
      "jedi                      0.13.2                   py36_0  \n",
      "jinja2                    2.10                     py36_0  \n",
      "jpeg                      9b                   he5867d9_2  \n",
      "jsonschema                2.6.0            py36hb385e00_0  \n",
      "jupyter                   1.0.0                    py36_7  \n",
      "jupyter_client            5.2.4                    py36_0  \n",
      "jupyter_console           6.0.0                    py36_0  \n",
      "jupyter_contrib_core      0.3.3                      py_2    conda-forge\n",
      "jupyter_contrib_nbextensions 0.5.0                 py36_1000    conda-forge\n",
      "jupyter_core              4.4.0                    py36_0  \n",
      "jupyter_highlight_selected_word 0.2.0                 py36_1000    conda-forge\n",
      "jupyter_latex_envs        1.4.4                 py36_1000    conda-forge\n",
      "jupyter_nbextensions_configurator 0.4.0                 py36_1000    conda-forge\n",
      "jupyterlab                0.34.9                   py36_0  \n",
      "jupyterlab_launcher       0.13.1                   py36_0  \n",
      "keras-applications        1.0.6                    py36_0  \n",
      "keras-preprocessing       1.0.5                    py36_0  \n",
      "kiwisolver                1.0.1            py36h0a44026_0  \n",
      "lazy-object-proxy         1.3.1            py36h1de35cc_2  \n",
      "libcxx                    4.0.1                hcfea43d_1  \n",
      "libcxxabi                 4.0.1                hcfea43d_1  \n",
      "libedit                   3.1.20181209         hb402a30_0  \n",
      "libffi                    3.2.1                h475c297_4  \n",
      "libgfortran               3.0.1                h93005f0_2  \n",
      "libiconv                  1.15                 hdd342a3_7  \n",
      "libopenblas               0.3.3                hdc02c5d_3  \n",
      "libpng                    1.6.36               ha441bb4_0  \n",
      "libprotobuf               3.6.1                hd9629dc_0  \n",
      "libsodium                 1.0.16               h3efe00b_0  \n",
      "libxml2                   2.9.9                hab757c2_0  \n",
      "libxslt                   1.1.33               h33a18ac_0  \n",
      "lxml                      4.3.0            py36hef8c89e_0  \n",
      "markdown                  3.0.1                    py36_0  \n",
      "markupsafe                1.1.0            py36h1de35cc_0  \n",
      "matplotlib                3.0.2            py36h54f8f79_0  \n",
      "mccabe                    0.6.1                    py36_1  \n",
      "mistune                   0.8.4            py36h1de35cc_0  \n",
      "more-itertools            5.0.0                    py36_0  \n",
      "nbconvert                 5.4.0                    py36_1  \n",
      "nbdime                    1.0.4                 py36_1000    conda-forge\n",
      "nbformat                  4.4.0            py36h827af21_0  \n",
      "ncurses                   6.1                  h0a44026_1  \n",
      "nomkl                     3.0                           0  \n",
      "notebook                  5.6.0                    py36_0  \n",
      "numpy                     1.14.5          py36_blas_openblashd3ea46f_202  [blas_openblas]  conda-forge\n",
      "openblas                  0.2.20                        8    conda-forge\n",
      "openssl                   1.0.2p               h1de35cc_0  \n",
      "pandas                    0.23.4           py36h6440ff4_0  \n",
      "pandoc                    1.19.2.1             ha5e8f32_1  \n",
      "pandocfilters             1.4.2                    py36_1  \n",
      "parso                     0.3.1                    py36_0  \n",
      "pcre                      8.42                 h378b8a2_0  \n",
      "pep8                      1.7.1                    py36_0  \n",
      "pexpect                   4.6.0                    py36_0  \n",
      "pickleshare               0.7.5                    py36_0  \n",
      "pillow                    5.4.1                    pypi_0    pypi\n",
      "pip                       18.0                  py36_1001    conda-forge\n",
      "pluggy                    0.6.0                    py36_0  \n",
      "prometheus_client         0.5.0                    py36_0  \n",
      "prompt_toolkit            2.0.7                    py36_0  \n",
      "protobuf                  3.6.1            py36h0a44026_0  \n",
      "ptyprocess                0.6.0                    py36_0  \n",
      "py                        1.7.0                    py36_0  \n",
      "pycodestyle               2.4.0                    py36_0  \n",
      "pycparser                 2.19                     py36_0  \n",
      "pydocstyle                3.0.0                    py36_0  \n",
      "pyflakes                  2.0.0                    py36_0  \n",
      "pygments                  2.3.1                    py36_0  \n",
      "pylama                    7.4.3                      py_0    conda-forge\n",
      "pylint                    2.2.2                    py36_0  \n",
      "pyopenssl                 18.0.0                   py36_0  \n",
      "pyparsing                 2.3.1                    py36_0  \n",
      "pyqt                      5.9.2            py36h655552a_2  \n",
      "pysocks                   1.6.8                    py36_0  \n",
      "pytest                    3.6.2                    py36_0  \n",
      "python                    3.6.6                hc167b69_0  \n",
      "python-dateutil           2.7.5                    py36_0  \n",
      "pytz                      2018.9                   py36_0  \n",
      "pyyaml                    3.13             py36h1de35cc_0  \n",
      "pyzmq                     17.1.2           py36h1de35cc_0  \n",
      "qrcode                    6.1                      pypi_0    pypi\n",
      "qt                        5.9.7                h468cd18_1  \n",
      "qtconsole                 4.4.3                    py36_0  \n",
      "readline                  7.0                  h1de35cc_5  \n",
      "requests                  2.21.0                   py36_0  \n",
      "scikit-learn              0.20.1           py36hebd9d1a_0  \n",
      "scipy                     1.1.0            py36h1a1e112_2  \n",
      "send2trash                1.5.0                    py36_0  \n",
      "setuptools                40.6.3                   py36_0  \n",
      "sip                       4.19.8           py36h0a44026_0  \n",
      "six                       1.12.0                   py36_0  \n",
      "smmap2                    2.0.5                    py36_0  \n",
      "snowballstemmer           1.2.1            py36h6c7b616_0  \n",
      "sqlite                    3.26.0               ha441bb4_0  \n",
      "tensorboard               1.12.0           py36hdc36e2c_0  \n",
      "tensorflow                1.12.0          mkl_py36h2b2bbaf_0  \n",
      "tensorflow-base           1.12.0          mkl_py36h70e0e9a_0  \n",
      "termcolor                 1.1.0                    py36_1  \n",
      "terminado                 0.8.1                    py36_1  \n",
      "testpath                  0.4.2                    py36_0  \n",
      "tk                        8.6.8                ha441bb4_0  \n",
      "toolz                     0.9.0                    pypi_0    pypi\n",
      "tornado                   4.5.3                    py36_0  \n",
      "traitlets                 4.3.2            py36h65bd3ce_0  \n",
      "typed-ast                 1.1.0            py36h1de35cc_0  \n",
      "urllib3                   1.24.1                   py36_0  \n",
      "vega                      1.4.0                    pypi_0    pypi\n",
      "vega-datasets             0.7.0                    pypi_0    pypi\n",
      "watermark                 1.8.0                      py_0    conda-forge\n",
      "wcwidth                   0.1.7            py36h8c6ec74_0  \n",
      "webencodings              0.5.1                    py36_1  \n",
      "werkzeug                  0.14.1                   py36_0  \n",
      "wheel                     0.32.3                   py36_0  \n",
      "widgetsnbextension        3.4.2                    py36_0  \n",
      "wrapt                     1.11.0           py36h1de35cc_0  \n",
      "xz                        5.2.4                h1de35cc_4  \n",
      "yaml                      0.1.7                hc338f04_2  \n",
      "zeromq                    4.2.5                h0a44026_1  \n",
      "zlib                      1.2.11               h1de35cc_3  \n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_gcp_dl]",
   "language": "python",
   "name": "conda-env-env_gcp_dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
